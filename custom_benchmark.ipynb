{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57f51bb443694561",
   "metadata": {},
   "source": [
    "# RAG Benchmark Evaluation on Natural Questions\n",
    "\n",
    "This notebook evaluates a RAG (Retrieval-Augmented Generation) system on the Natural Questions dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea118ea9e434089f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Setup and Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60407f4d1af3bd96",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Imports and dependencies\n",
    "\n",
    "Import all necessary libraries for dataset loading, embeddings, LLM, text processing, and evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "ea4eea478c45fbb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T16:33:00.463992Z",
     "start_time": "2025-07-02T16:33:00.459481Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import evaluate\n",
    "\n",
    "from datetime import datetime\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.document import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841ce3ea6db11221",
   "metadata": {},
   "source": [
    "### Config File\n",
    "\n",
    "Configure the dataset split, output folder, embedding model, chunk size, overlap, and LLM parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "5cf0ebd2fe01be71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T16:33:00.547995Z",
     "start_time": "2025-07-02T16:33:00.544881Z"
    }
   },
   "outputs": [],
   "source": [
    "config_file = \"configs/custom_benchmark_config.json\"\n",
    "\n",
    "with open(config_file, \"r\") as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872e3764aa05cf93",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load API Key\n",
    "\n",
    "Load the Hugging Face API key from environment variables to authenticate model requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "90e3df368ffe7436",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T16:33:00.567283Z",
     "start_time": "2025-07-02T16:33:00.561242Z"
    }
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "HF_API_KEY = os.getenv(\"API_KEY7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d918d513f3e953",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load dataset\n",
    "\n",
    "Load the Natural Questions dataset subset for benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "292d18be6927d28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T16:33:05.613036Z",
     "start_time": "2025-07-02T16:33:00.568727Z"
    },
    "is_executing": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26ed514cc0fb4b17bc1cdbae479217de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/287 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7254796707c44d448e400d650c71b28f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/287 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"natural_questions\", split=config[\"dataset_split\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8a0c45ab8fd77c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Model Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ec0c6bbede92a4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Embeddings initialization\n",
    "\n",
    "Create the embedding model for converting text chunks into vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "733553b79138a2ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T16:33:07.494706Z",
     "start_time": "2025-07-02T16:33:05.614373Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=config[\"embedding_model_name\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8989ddb91f0dfa0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Text splitter configuration\n",
    "\n",
    "Define how the document is split into overlapping chunks for embedding and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "c4ec7696c36ded36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T16:33:07.499314Z",
     "start_time": "2025-07-02T16:33:07.494706Z"
    },
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=config[\"chunk_size\"],\n",
    "    chunk_overlap=config[\"chunk_overlap\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75658f377b18de7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### LLM setup\n",
    "\n",
    "Configure the language model endpoint on Hugging Face Hub for text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "604630da582b0af3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T16:33:07.505915Z",
     "start_time": "2025-07-02T16:33:07.501012Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=config[\"llm_model\"],\n",
    "    huggingfacehub_api_token=HF_API_KEY,\n",
    "    task=\"text-generation\",\n",
    "    temperature=config[\"temperature\"],\n",
    "    max_new_tokens=config[\"max_new_tokens\"],\n",
    ")\n",
    "chat_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9983a91bdc30f78",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Retrieval and querying\n",
    "\n",
    "Functions to retrieve relevant documents locally and query the LLM with that context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "20b6e3f8315f79b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T16:33:07.510654Z",
     "start_time": "2025-07-02T16:33:07.506433Z"
    },
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def retrieve_local(query, vectorstore, k=config[\"top_k\"]):\n",
    "    docs_faiss = vectorstore.similarity_search(query, k=k)\n",
    "    return [d.page_content for d in docs_faiss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "80d2661109411a70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T16:33:07.516246Z",
     "start_time": "2025-07-02T16:33:07.511663Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def ask(query, context):\n",
    "    messages = [\n",
    "        SystemMessage(content=f\"Just answer queries based on {context}.\"),\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        Answer the query: {query} based uniquely on the context: {context}, don't make up anything, just say what the context contains. If the information is not in the context, you must say you don't know. You must answer only the specified question and nothing else.\n",
    "        \"\"\")\n",
    "    ]\n",
    "    response = chat_model.invoke(messages)\n",
    "    return response.content\n",
    "\n",
    "#ask(\"What is the capital of France?\", \"France is a country in Europe.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2ebed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_check(query, ground_truth, prediction):\n",
    "    messages = [\n",
    "        #SystemMessage(content=f\"Just answer queries based on {ground_truth}.\"),\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        {ground_truth}\n",
    "\n",
    "        given the previous statements, you MUST say (shortly) if the following phrase is correct\n",
    "\n",
    "        {prediction}\n",
    "        \"\"\")\n",
    "    ]\n",
    "    response = chat_model.invoke(messages)\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9216a27f27bb44",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data extraction and preprocessing\n",
    "\n",
    "Functions to extract valid answers from dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "1dd55c4e0a2d91e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T16:33:07.525093Z",
     "start_time": "2025-07-02T16:33:07.517256Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def extract_answers(sample):\n",
    "    tokens = sample[\"document\"][\"tokens\"]\n",
    "    short_answer = \"\"\n",
    "    start = sample[\"annotations\"][\"short_answers\"][0][\"start_token\"]\n",
    "    end = sample[\"annotations\"][\"short_answers\"][0][\"end_token\"]\n",
    "    if len(start) > 0:\n",
    "        short_answer = \" \".join([\n",
    "            t for t, html in zip(tokens[\"token\"][int(start[0]):int(end[0])], tokens[\"is_html\"][start[0]:end[0]]) if not html\n",
    "        ])\n",
    "\n",
    "    long_answer = \"\"\n",
    "    if sample[\"annotations\"][\"long_answer\"][0][\"start_token\"] != -1:\n",
    "        start = sample[\"annotations\"][\"long_answer\"][0][\"start_token\"]\n",
    "        end = sample[\"annotations\"][\"long_answer\"][0][\"end_token\"]\n",
    "        long_answer = \" \".join([\n",
    "            t for t, html in zip(tokens[\"token\"][start:end], tokens[\"is_html\"][start:end]) if not html\n",
    "        ])\n",
    "\n",
    "    return long_answer or \"\", short_answer or \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "b2fa481a60693a36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T16:33:07.531469Z",
     "start_time": "2025-07-02T16:33:07.527139Z"
    },
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def preprocess_text(sample):\n",
    "    tokens = sample[\"document\"][\"tokens\"]\n",
    "    return \" \".join([t for t, html in zip(tokens[\"token\"], tokens[\"is_html\"]) if not html])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b739fec4387fcc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Benchmarking "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5901ab8d628195c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Retrieve benchmark\n",
    "\n",
    "By finding the longest match between the prediction and the golden context, we can evaluate how well the model retrieves relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "5b2c257fcdb1d26b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T16:33:07.538105Z",
     "start_time": "2025-07-02T16:33:07.532475Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def find_longest_match(str1, str2):\n",
    "    max_len = 0\n",
    "    len2 = len(str2)\n",
    "\n",
    "    for i in range(len2):\n",
    "        for j in range(i + 1, len2 + 1):\n",
    "            substr = str2[i:j]\n",
    "            if substr in str1 and len(substr) > max_len:\n",
    "                max_len = len(substr)\n",
    "\n",
    "    return max_len / len2 if len2 > 0 else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5dc984b0153086",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Evaluation benchmark\n",
    "\n",
    "Load rouge and bleu metrics for evaluating the model's predictions against the golden answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "b76c5209da15957e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T16:33:10.160892Z",
     "start_time": "2025-07-02T16:33:07.540605Z"
    },
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint C:\\Users\\ruben\\.cache\\huggingface\\metrics\\bleurt\\bleurt-large-512\\downloads\\extracted\\b2c310c50992e604abf5e8f9f4695388f5f2e7bfdfc3d5d81a5f4693d2f300d3\\bleurt-large-512.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint C:\\Users\\ruben\\.cache\\huggingface\\metrics\\bleurt\\bleurt-large-512\\downloads\\extracted\\b2c310c50992e604abf5e8f9f4695388f5f2e7bfdfc3d5d81a5f4693d2f300d3\\bleurt-large-512.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint bert_custom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint bert_custom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:bert_custom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:bert_custom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:vocab.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating WordPiece tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating WordPiece tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    }
   ],
   "source": [
    "metric_rouge = evaluate.load(\"rouge\")\n",
    "metric_bleu = evaluate.load(\"bleu\")\n",
    "metric_bleurt = evaluate.load(\"bleurt\", \"bleurt-large-512\")\n",
    "comparison_model = SentenceTransformer(config[\"embedding_model_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72a8cb8659c09c9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Single sample processing\n",
    "\n",
    "Process one sample: build context, eventually query LLM, extract answer, and compute metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "91dd8c46c6a1cb9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T16:33:10.168497Z",
     "start_time": "2025-07-02T16:33:10.161917Z"
    },
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def process_sample(i, sample):\n",
    "    golden_context, golden_answer = extract_answers(sample)\n",
    "    if golden_answer == \"\" or golden_context == \"\":\n",
    "        return None\n",
    "    golden_embeddings = comparison_model.encode(golden_context, convert_to_tensor=True)\n",
    "    \n",
    "    query = sample[\"question\"][\"text\"]\n",
    "\n",
    "    text = preprocess_text(sample)\n",
    "    docs = [Document(page_content=text)]\n",
    "    split_docs = splitter.split_documents(docs)\n",
    "    vectorstore = FAISS.from_documents(split_docs, embedding_model)\n",
    "    context = retrieve_local(query, vectorstore)\n",
    "    \n",
    "    prediction = ask(query, context)\n",
    "        \n",
    "    rouge = metric_rouge.compute(predictions=[prediction], references=[golden_context])\n",
    "    bleu = metric_bleu.compute(predictions=[prediction], references=[golden_context])\n",
    "    bleurt = metric_bleurt.compute(predictions=[prediction], references=[golden_context])\n",
    "    longest_match = find_longest_match(prediction, golden_context)\n",
    "    pred_embeddings = comparison_model.encode(prediction, convert_to_tensor=True)\n",
    "    cosine_similarity = util.pytorch_cos_sim(pred_embeddings, golden_embeddings)\n",
    "    semantic_evaluation = semantic_check(query, golden_context, prediction)\n",
    "\n",
    "    prediction_golden = ask(query, golden_context)\n",
    "        \n",
    "    rouge_golden = metric_rouge.compute(predictions=[prediction_golden], references=[golden_context])\n",
    "    bleu_golden = metric_bleu.compute(predictions=[prediction_golden], references=[golden_context])\n",
    "    bleurt_golden = metric_bleurt.compute(predictions=[prediction_golden], references=[golden_context])\n",
    "    longest_match_golden = find_longest_match(prediction_golden, golden_context)\n",
    "    pred_embeddings_golden = comparison_model.encode(prediction_golden, convert_to_tensor=True)\n",
    "    cosine_similarity_golden = util.pytorch_cos_sim(pred_embeddings_golden, golden_embeddings)\n",
    "    semantic_evaluation_golden = semantic_check(query, golden_context, prediction_golden)\n",
    "    \n",
    "    result = {\n",
    "        \"index\": i,\n",
    "        \"rougeL\": rouge[\"rougeL\"],\n",
    "        \"rougeL_golden\": rouge_golden[\"rougeL\"],\n",
    "        \"bleu\": bleu[\"bleu\"],\n",
    "        \"bleu_golden\": bleu_golden[\"bleu\"],\n",
    "        \"bleurt\": bleurt[\"scores\"][0],\n",
    "        \"bleurt_golden\": bleurt_golden[\"scores\"][0],\n",
    "        \"cosine_similarity\": cosine_similarity.item(),\n",
    "        \"cosine_similarity_golden\": cosine_similarity_golden.item(),\n",
    "        \"longest_match\": longest_match,\n",
    "        \"longest_match_golden\": longest_match_golden,\n",
    "        \"query\": query,\n",
    "        \"gold_answer\": golden_answer,\n",
    "        \"golden_context\": golden_context,\n",
    "        \"prediction\": prediction,\n",
    "        \"prediction_golden\": prediction_golden,\n",
    "        \"semantic_evaluation\": semantic_evaluation,\n",
    "        \"semantic_evaluation_golden\": semantic_evaluation_golden\n",
    "    }\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b8db3a1ad908b0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Full benchmark loop\n",
    "\n",
    "Run the evaluation until the target number of valid samples with answers is reached, skipping samples with empty references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "c273ca2913c053a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T16:33:29.273565Z",
     "start_time": "2025-07-02T16:33:10.169799Z"
    },
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf5403fdc9d4b11afd47b02fa398ccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3 valid samples out of 17 total samples\n"
     ]
    }
   ],
   "source": [
    "num_valid_examples = config[\"num_valid_examples\"]\n",
    "\n",
    "results = []\n",
    "i = 0\n",
    "\n",
    "with tqdm(total=num_valid_examples) as pbar:\n",
    "    while len(results) < num_valid_examples and i < len(dataset):\n",
    "        sample = dataset[i]\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        result = process_sample(i, sample)\n",
    "        elapsed_time = time.perf_counter() - start_time\n",
    "            \n",
    "        if result:\n",
    "            result[\"elapsed_time\"] = elapsed_time\n",
    "            results.append(result)\n",
    "            pbar.update(1)\n",
    "            \n",
    "        i += 1\n",
    "\n",
    "print(f\"Processed {len(results)} valid samples out of {i} total samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2581af0cde6a2c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Save and display results\n",
    "\n",
    "Print average metric scores, save data to CSV and JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "34638e2b890f8ac6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T16:33:29.280839Z",
     "start_time": "2025-07-02T16:33:29.274614Z"
    },
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===================================== SAMPLE 11 =====================================\n",
      "QUERY: when did now thats what i call music come out\n",
      "\n",
      "GROUND TRUTH: Now That 's What I Call Music ( also simply titled Now or Now 1 ) is the first album from the popular Now ! series that was released in the United Kingdom on 28 November 1983 . Initial pressings were released on vinyl and audio cassette . To celebrate the 25th anniversary of the album and series , the album was re-released on CD for the first time in 2009 . However , alternative longer mixes of Only For Love , Double Dutch and Candy Girl were included in place of the original shorter single mixes from 1983 . A double vinyl re-release followed for Record Store Day on 18 April 2015 . In July 2018 , the album was newly remastered and re-released on CD , vinyl and cassette to commemorate the release of the 100th volume of the series .\n",
      "\n",
      "GOLDEN ANSWER: 28 November 1983\n",
      "\n",
      "PREDICTION: The context provided states that \"Now That's What I Call Music ( 1983 ) Now That's What I Call Music II ( 1984 ) Now That's What I Call Music ( also simply titled Now or Now 1 ) is the first album from the popular Now ! series that was released in the United Kingdom on 28 November 1983.\" This information specifically pertains to the release date of the album, which was 28 November 1983. Therefore, the answer to the question \"When did Now That's What I Call Music come out?\" is 28 November 1983.\n",
      "\n",
      "PREDICTION SEMANTIC EVALUATION: 28 November 1983.\n",
      "PREDICTION GOLDEN: Now That's What I Call Music (also simply titled Now or Now 1) was released in the United Kingdom on 28 November 1983.\n",
      "\n",
      "PREDICTION GOLDEN SEMANTIC EVALUATION: Yes\n",
      "rougeL: 0.39111111111111113, rougeL_golden: 0.3076923076923077\n",
      "bleu: 0.25762089679581407, bleu_golden: 0.009574762725275736\n",
      "bleurt: -0.6021697521209717, bleurt_golden: -0.5207092761993408\n",
      "cosine_similarity: 0.8175095915794373, cosine_similarity_golden: 0.777004063129425\n",
      "longest match: 0.22297297297297297, longest_match_golden: 0.07432432432432433\n",
      "\n",
      "\n",
      "===================================== SAMPLE 13 =====================================\n",
      "QUERY: where is the 2018 grey cup being played\n",
      "\n",
      "GROUND TRUTH: The game is scheduled to be played at The Brick Field at Commonwealth Stadium in Edmonton , Alberta on Sunday , November 25 , 2018 .\n",
      "\n",
      "GOLDEN ANSWER: The Brick Field at Commonwealth Stadium in Edmonton , Alberta\n",
      "\n",
      "PREDICTION: The 2018 Grey Cup will be played in Edmonton, Alberta, specifically at The Brick Field at Commonwealth Stadium, which was officially named the host city on March 24, 2017, when multiple news outlets reported that the Calgary Stampeders and Edmonton Eskimos were bidding to host the game. Prior to this announcement, the Grey Cup had been awarded based on regional preference or ownership changes (such as Toronto hosting the 104th Grey Cup four years after hosting their previous one), but the league has since adopted a new approach based on merit applications to determine a host. This change comes as Calgary last hosted the Grey Cup in 2009 while Edmonton last hosted in 2010, both of which occurred behind schedule. The team's CEO, Scott Mitchell, stated that the organization is currently focused on resolving legal disputes before focusing on hosting the Grey Cup in the near future.\n",
      "\n",
      "PREDICTION SEMANTIC EVALUATION: Yes, the statement is correct. The 2018 Grey Cup will be played in Edmonton, Alberta at The Brick Field at Commonwealth Stadium, which was officially named the host city on March 24, 2017. Prior to this announcement, the Grey Cup had been awarded based on regional preference or ownership changes, but the league has since adopted a new approach based on merit applications to determine a host. This change comes as Calgary last hosted the Grey Cup in 2009 while Edmonton last hosted in 2010, both of which occurred behind schedule. The team's CEO, Scott Mitchell, stated that the organization is currently focused on resolving legal disputes before focusing on hosting the Grey Cup in the near future.\n",
      "PREDICTION GOLDEN: The 2018 Grey Cup will be played at The Brick Field at Commonwealth Stadium in Edmonton, Alberta, on Sunday, November 25, 2018. This is unique information that can be inferred from the given context as it specifies the location of the event and a specific date, which are both crucial details for knowing where the Grey Cup takes place. In this case, the context provides information about the location (The Brick Field) and the time (Sunday, November 25, 2018), making it clear that the 2018 Grey Cup is taking place in an indoor venue located in Edmonton, Alberta. Therefore, we can confidently determine that the context contains the following details:\n",
      "\n",
      "- The location: The Brick Field at Commonwealth Stadium\n",
      "- The date: Sunday, November 25, 2018\n",
      "\n",
      "Without any additional context or information, we cannot make any assumptions or guesses about other factors such as the stadium's capacity, historical significance,或者其他 players' teams, the score, or the outcome of the game. However, these details alone provide the necessary information to identify where the 2018 Grey Cup will take place during its scheduled gameplay.\n",
      "\n",
      "PREDICTION GOLDEN SEMANTIC EVALUATION: Yes, the given context correctly states that the 2018 Grey Cup will be played at The Brick Field at Commonwealth Stadium in Edmonton, Alberta, on Sunday, November 25, 2018. This information is evident from the mention of the venue name \"The Brick Field\" and the specific date \"Sunday, November 25, 2018.\" The fact that it is an indoor stadium also suggests that the game will likely take place during a winter season when outdoor football games may not be feasible due to colder weather conditions. Additionally, The Brick Field has a history of hosting major sporting events, including the Canadian Football League (CFL) Grey Cup, further emphasizing its importance and credibility as the site of the annual championship match. Overall, based solely on the provided context, we can confidently conclude that the 2018 Grey Cup will be held at The Brick Field at Commonwealth Stadium in Edmonton, Alberta on Sunday, November 25, 2018.\n",
      "rougeL: 0.15294117647058825, rougeL_golden: 0.17821782178217824\n",
      "bleu: 0.058004349660335894, bleu_golden: 0.08907636347549949\n",
      "bleurt: -0.27187347412109375, bleurt_golden: 0.06464247405529022\n",
      "cosine_similarity: 0.5242590308189392, cosine_similarity_golden: 0.7347730994224548\n",
      "longest match: 0.32575757575757575, longest_match_golden: 0.49242424242424243\n",
      "\n",
      "\n",
      "===================================== SAMPLE 16 =====================================\n",
      "QUERY: where did the band bastille get their name\n",
      "\n",
      "GROUND TRUTH: Bastille ( stylised as BΔSTILLE ) are a British rock band formed in 2010 . The group began as a solo project by lead vocalist Dan Smith , but later expanded to include keyboardist Kyle Simmons , bassist and guitarist Will Farquarson , and drummer Chris Wood . The name of the band derives from Bastille Day , which is celebrated on 14 July , the date of Smith 's birthday .\n",
      "\n",
      "GOLDEN ANSWER: Bastille Day\n",
      "\n",
      "PREDICTION: The context contains information about the band Bastille, which was formed in 2010 as a solo project by lead vocalist Dan Smith. The band's name, Bastille, is unique because it is a stylized version of the French word \"bataille,\" which means \"battle\" or \"fight.\" This naming convention reflects the group's commitment to using music as a form of political and social commentary, as well as their desire to create music that resonates with audiences across different cultures and backgrounds. Bastille has gained popularity for their eclectic mix of genres, including indie pop, synth-pop, and indie rock, and has released several critically acclaimed albums throughout their career. Their songs often tackle topics such as war, politics, and consumerism, and have been praised for their catchy melodies and thought-provoking lyrics. Bastille's meteoric rise to fame began in 2014 when they performed at Coachella festival, becoming one of the most popular acts of the year. Since then, the band has sold over 8 million records worldwide, won numerous awards, and collaborated with other artists, including Eric Prydz and Emeli Sandé. Bastille continues to release new music and perform live shows around the world, maintaining their relevance and impact in the ever-changing landscape of modern music.\n",
      "\n",
      "PREDICTION SEMANTIC EVALUATION: Correct.\n",
      "PREDICTION GOLDEN: Bastille's name is derived uniquely from the context of the band being formed in 2010, specifically celebrating Bastille Day on 14 July, the date of Dan Smith's birthday. The band began as a solo project by lead vocalist Dan Smith, but later expanded to include keyboardist Kyle Simmons, bassist and guitarist Will Farquarson, and drummer Chris Wood. This specific combination of musicians and their shared birthday on July 14 aligns with the cultural significance of Bastille Day, which commemorates the storming of the Bastille prison in Paris during the French Revolution. By using the English word \"bastille\" as its name, Bastille not only pays tribute to the revolutionary event but also incorporates the blend of musical elements and the specific date associated with it.\n",
      "\n",
      "PREDICTION GOLDEN SEMANTIC EVALUATION: Yes, the phrase \"Bastille's name is derived uniquely from the context of the band being formed in 2010, specifically celebrating Bastille Day on 14 July, the date of Dan Smith's birthday.\" accurately describes the origin of the band's name. The band started as a solo project by Dan Smith, but as they progressed, the members included Kyle Simmons, Will Farquarson, and Chris Wood. This unique combination of musicians and their shared birthday on July 14 resonated with the cultural significance of Bastille Day, which commemorates the storming of the Bastille prison in Paris during the French Revolution. Using the word \"bastille\" as its name, Bastille not only honors the historical event but also nods to the diverse musical influences and merging of genres that emerged from this collaborative effort.\n",
      "rougeL: 0.16974169741697415, rougeL_golden: 0.42328042328042326\n",
      "bleu: 0.05694260968660882, bleu_golden: 0.3139135052175141\n",
      "bleurt: -0.3073261082172394, bleurt_golden: 0.08520692586898804\n",
      "cosine_similarity: 0.7192693948745728, cosine_similarity_golden: 0.8748298287391663\n",
      "longest match: 0.12064343163538874, longest_match_golden: 0.15013404825737264\n"
     ]
    }
   ],
   "source": [
    "for r in results:\n",
    "    print(f\"\\n\\n===================================== SAMPLE {r['index']} =====================================\")\n",
    "    print(f\"QUERY: {r['query']}\\n\")\n",
    "    print(f\"GROUND TRUTH: {r['golden_context']}\\n\")\n",
    "    print(f\"GOLDEN ANSWER: {r['gold_answer']}\\n\")\n",
    "    print(f\"PREDICTION: {r['prediction']}\\n\")\n",
    "    print(f\"PREDICTION SEMANTIC EVALUATION: {r['semantic_evaluation']}\")\n",
    "    print(f\"PREDICTION GOLDEN: {r['prediction_golden']}\\n\")\n",
    "    print(f\"PREDICTION GOLDEN SEMANTIC EVALUATION: {r['semantic_evaluation_golden']}\")\n",
    "    print(f\"rougeL: {r['rougeL']}, rougeL_golden: {r['rougeL_golden']}\")\n",
    "    print(f\"bleu: {r['bleu']}, bleu_golden: {r['bleu_golden']}\")\n",
    "    print(f\"bleurt: {r['bleurt']}, bleurt_golden: {r['bleurt_golden']}\")\n",
    "    print(f\"cosine_similarity: {r['cosine_similarity']}, cosine_similarity_golden: {r['cosine_similarity_golden']}\")\n",
    "    print(f\"longest match: {r['longest_match']}, longest_match_golden: {r['longest_match_golden']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "258f9d60c1895cf1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T16:33:29.313904Z",
     "start_time": "2025-07-02T16:33:29.281900Z"
    },
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "output_folder = config[\"output_folder\"]\n",
    "run_name = f\"run_custom_{datetime.now().strftime(\"%Y_%m_%d_%H%M%S\")}\"\n",
    "out_path = os.path.join(output_folder, run_name)\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "response = [\n",
    "    {\n",
    "        \"index\": row[\"index\"],\n",
    "        \"query\": row[\"query\"],\n",
    "        \"gold_answer\": row[\"gold_answer\"],\n",
    "        \"prediction\": row[\"prediction\"],\n",
    "        \"semantic_evaluation\": row[\"semantic_evaluation\"],\n",
    "        \"golden_context\": row[\"golden_context\"],\n",
    "        \"semantic_evaluation_golden\": row[\"semantic_evaluation_golden\"],\n",
    "        \"elapsed_time\": row[\"elapsed_time\"]\n",
    "    }\n",
    "    for _, row in results_df.iterrows()\n",
    "]\n",
    "results_df.drop([\"prediction_golden\", \"prediction\", \"query\", \"gold_answer\", \"elapsed_time\"], axis=1, inplace=True)\n",
    "\n",
    "file_path = os.path.join(out_path, f\"results.csv\")\n",
    "results_df.to_csv(file_path, index=False)\n",
    "\n",
    "with open(os.path.join(out_path, \"responses.json\"), \"w\") as f:\n",
    "    json.dump(response, f, indent=4)\n",
    "    \n",
    "with open(os.path.join(out_path, \"used_config.json\"), \"w\") as f:\n",
    "    json.dump(config, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
