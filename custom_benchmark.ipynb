{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# RAG Benchmark Evaluation on Natural Questions\n",
    "\n",
    "This notebook evaluates a RAG (Retrieval-Augmented Generation) system on the Natural Questions dataset."
   ],
   "id": "57f51bb443694561"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup and Initialization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea118ea9e434089f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports and dependencies\n",
    "\n",
    "Import all necessary libraries for dataset loading, embeddings, LLM, text processing, and evaluation metrics."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "60407f4d1af3bd96"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T16:28:42.400675Z",
     "start_time": "2025-06-23T16:28:42.396542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import evaluate\n",
    "\n",
    "from datetime import datetime\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.document import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv"
   ],
   "id": "ea4eea478c45fbb0",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Config File\n",
    "\n",
    "Configure the dataset split, output folder, embedding model, chunk size, overlap, and LLM parameters."
   ],
   "id": "841ce3ea6db11221"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T16:28:42.457913Z",
     "start_time": "2025-06-23T16:28:42.454375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config_file = \"configs/custom_benchmark_config.json\"\n",
    "\n",
    "with open(config_file, \"r\") as f:\n",
    "    config = json.load(f)"
   ],
   "id": "5cf0ebd2fe01be71",
   "outputs": [],
   "execution_count": 53
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load API Key\n",
    "\n",
    "Load the Hugging Face API key from environment variables to authenticate model requests."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "872e3764aa05cf93"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T16:28:42.462666Z",
     "start_time": "2025-06-23T16:28:42.459294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_dotenv()\n",
    "HF_API_KEY = os.getenv(\"API_KEY2\")"
   ],
   "id": "90e3df368ffe7436",
   "outputs": [],
   "execution_count": 54
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load dataset\n",
    "\n",
    "Load the Natural Questions dataset subset for benchmarking."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70d918d513f3e953"
  },
  {
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "end_time": "2025-06-23T16:28:49.286470Z",
     "start_time": "2025-06-23T16:28:42.465467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = load_dataset(\"natural_questions\", split=config[\"dataset_split\"])"
   ],
   "id": "292d18be6927d28",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Resolving data files:   0%|          | 0/287 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c945349843284bdb878bd9949f8a8bf0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Resolving data files:   0%|          | 0/287 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ede6b9759822403ab9183d352222d2fb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 55
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Initialization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae8a0c45ab8fd77c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Embeddings initialization\n",
    "\n",
    "Create the embedding model for converting text chunks into vector representations."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74ec0c6bbede92a4"
  },
  {
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "end_time": "2025-06-23T16:28:50.856553Z",
     "start_time": "2025-06-23T16:28:49.288237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=config[\"embedding_model_name\"]\n",
    ")"
   ],
   "id": "733553b79138a2ce",
   "outputs": [],
   "execution_count": 56
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Text splitter configuration\n",
    "\n",
    "Define how the document is split into overlapping chunks for embedding and retrieval."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8989ddb91f0dfa0"
  },
  {
   "cell_type": "code",
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=config[\"chunk_size\"],\n",
    "    chunk_overlap=config[\"chunk_overlap\"]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "end_time": "2025-06-23T16:28:50.860808Z",
     "start_time": "2025-06-23T16:28:50.858096Z"
    }
   },
   "id": "c4ec7696c36ded36",
   "outputs": [],
   "execution_count": 57
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LLM setup\n",
    "\n",
    "Configure the language model endpoint on Hugging Face Hub for text generation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f75658f377b18de7"
  },
  {
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "end_time": "2025-06-23T16:28:50.866589Z",
     "start_time": "2025-06-23T16:28:50.862653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=config[\"llm_model\"],\n",
    "    huggingfacehub_api_token=HF_API_KEY,\n",
    "    task=\"text-generation\",\n",
    "    temperature=config[\"temperature\"],\n",
    "    max_new_tokens=config[\"max_new_tokens\"],\n",
    ")\n",
    "chat_model = ChatHuggingFace(llm=llm)"
   ],
   "id": "604630da582b0af3",
   "outputs": [],
   "execution_count": 58
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Retrieval and querying\n",
    "\n",
    "Functions to retrieve relevant documents locally and query the LLM with that context."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9983a91bdc30f78"
  },
  {
   "cell_type": "code",
   "source": [
    "def retrieve_local(query, vectorstore, k=config[\"top_k\"]):\n",
    "    docs_faiss = vectorstore.similarity_search(query, k=k)\n",
    "    return [d.page_content for d in docs_faiss]"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "end_time": "2025-06-23T16:28:50.871733Z",
     "start_time": "2025-06-23T16:28:50.867784Z"
    }
   },
   "id": "20b6e3f8315f79b6",
   "outputs": [],
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "source": [
    "def ask(query, context):\n",
    "    messages = [\n",
    "        SystemMessage(content=f\"Just answer queries based on {context}.\"),\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        Answer the query: {query} based uniquely on the context: {context}, don't make up anything, just say what the context contains. If the information is not in the context, you must say you don't know. You must answer only the specified question and nothing else.\n",
    "        \"\"\")\n",
    "    ]\n",
    "    response = chat_model.invoke(messages)\n",
    "    return response.content"
   ],
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "end_time": "2025-06-23T16:28:50.876744Z",
     "start_time": "2025-06-23T16:28:50.872743Z"
    }
   },
   "id": "80d2661109411a70",
   "outputs": [],
   "execution_count": 60
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data extraction and preprocessing\n",
    "\n",
    "Functions to extract valid answers from dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d9216a27f27bb44"
  },
  {
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "end_time": "2025-06-23T16:28:50.883285Z",
     "start_time": "2025-06-23T16:28:50.877755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_answers(sample):\n",
    "    tokens = sample[\"document\"][\"tokens\"]\n",
    "    short_answer = \"\"\n",
    "    start = sample[\"annotations\"][\"short_answers\"][0][\"start_token\"]\n",
    "    end = sample[\"annotations\"][\"short_answers\"][0][\"end_token\"]\n",
    "    if len(start) > 0:\n",
    "        short_answer = \" \".join([\n",
    "            t for t, html in zip(tokens[\"token\"][int(start[0]):int(end[0])], tokens[\"is_html\"][start[0]:end[0]]) if not html\n",
    "        ])\n",
    "\n",
    "    long_answer = \"\"\n",
    "    if sample[\"annotations\"][\"long_answer\"][0][\"start_token\"] != -1:\n",
    "        start = sample[\"annotations\"][\"long_answer\"][0][\"start_token\"]\n",
    "        end = sample[\"annotations\"][\"long_answer\"][0][\"end_token\"]\n",
    "        long_answer = \" \".join([\n",
    "            t for t, html in zip(tokens[\"token\"][start:end], tokens[\"is_html\"][start:end]) if not html\n",
    "        ])\n",
    "\n",
    "    return long_answer or \"\", short_answer or \"\""
   ],
   "id": "1dd55c4e0a2d91e1",
   "outputs": [],
   "execution_count": 61
  },
  {
   "cell_type": "code",
   "source": [
    "def preprocess_text(sample):\n",
    "    tokens = sample[\"document\"][\"tokens\"]\n",
    "    return \" \".join([t for t, html in zip(tokens[\"token\"], tokens[\"is_html\"]) if not html])"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "end_time": "2025-06-23T16:28:50.889107Z",
     "start_time": "2025-06-23T16:28:50.884292Z"
    }
   },
   "id": "b2fa481a60693a36",
   "outputs": [],
   "execution_count": 62
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Benchmarking "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b739fec4387fcc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Retrieve benchmark\n",
    "\n",
    "By finding the longest match between the prediction and the golden context, we can evaluate how well the model retrieves relevant information."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5901ab8d628195c"
  },
  {
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "end_time": "2025-06-23T16:28:50.894864Z",
     "start_time": "2025-06-23T16:28:50.890115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def find_longest_match(str1, str2):\n",
    "    max_len = 0\n",
    "    len2 = len(str2)\n",
    "\n",
    "    for i in range(len2):\n",
    "        for j in range(i + 1, len2 + 1):\n",
    "            substr = str2[i:j]\n",
    "            if substr in str1 and len(substr) > max_len:\n",
    "                max_len = len(substr)\n",
    "\n",
    "    return max_len / len2 if len2 > 0 else 0\n"
   ],
   "id": "5b2c257fcdb1d26b",
   "outputs": [],
   "execution_count": 63
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation benchmark\n",
    "\n",
    "Load rouge and bleu metrics for evaluating the model's predictions against the golden answers. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b5dc984b0153086"
  },
  {
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "end_time": "2025-06-23T16:28:52.874359Z",
     "start_time": "2025-06-23T16:28:50.897377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "metric_rouge = evaluate.load(\"rouge\")\n",
    "metric_bleu = evaluate.load(\"bleu\")"
   ],
   "id": "b76c5209da15957e",
   "outputs": [],
   "execution_count": 64
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Single sample processing\n",
    "\n",
    "Process one sample: build context, eventually query LLM, extract answer, and compute metrics."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d72a8cb8659c09c9"
  },
  {
   "cell_type": "code",
   "source": [
    "def process_sample(i, sample, use_llm):\n",
    "    golden_context, golden_answer = extract_answers(sample)\n",
    "    if golden_answer == \"\" or golden_context == \"\":\n",
    "        return None\n",
    "    \n",
    "    query = sample[\"question\"][\"text\"]\n",
    "    \n",
    "    # Standard request\n",
    "    text = preprocess_text(sample)\n",
    "    docs = [Document(page_content=text)]\n",
    "    split_docs = splitter.split_documents(docs)\n",
    "    vectorstore = FAISS.from_documents(split_docs, embedding_model)\n",
    "    context = retrieve_local(query, vectorstore)\n",
    "    \n",
    "    if use_llm:\n",
    "        prediction = ask(query, context)\n",
    "    else:   \n",
    "        prediction = context[0]\n",
    "        \n",
    "    rouge = metric_rouge.compute(predictions=[prediction], references=[golden_context])\n",
    "    bleu = metric_bleu.compute(predictions=[prediction], references=[golden_context])\n",
    "    longest_match = find_longest_match(prediction, golden_context)\n",
    "    \n",
    "    # Golden answer scenario\n",
    "    if use_llm:\n",
    "        prediction_golden = ask(query, golden_context)\n",
    "    else:\n",
    "        prediction_golden = golden_context\n",
    "        \n",
    "    rouge_golden = metric_rouge.compute(predictions=[prediction_golden], references=[golden_context])\n",
    "    bleu_golden = metric_bleu.compute(predictions=[prediction_golden], references=[golden_context])\n",
    "    longest_match_golden = find_longest_match(prediction_golden, golden_context)\n",
    "\n",
    "    result = {\n",
    "        \"index\": i,\n",
    "        \"rougeL\": rouge[\"rougeL\"],\n",
    "        \"rougeL_golden\": rouge_golden[\"rougeL\"],\n",
    "        \"bleu\": bleu[\"bleu\"],\n",
    "        \"bleu_golden\": bleu_golden[\"bleu\"],\n",
    "        \"longest_match\": longest_match,\n",
    "        \"longest_match_golden\": longest_match_golden,\n",
    "        \"query\": query,\n",
    "        \"gold_answer\": golden_answer,\n",
    "        \"prediction\": prediction,\n",
    "        \"prediction_golden\": prediction_golden,\n",
    "    }\n",
    "    return result\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "end_time": "2025-06-23T16:28:52.881793Z",
     "start_time": "2025-06-23T16:28:52.875366Z"
    }
   },
   "id": "91dd8c46c6a1cb9e",
   "outputs": [],
   "execution_count": 65
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Full benchmark loop\n",
    "\n",
    "Run the evaluation until the target number of valid samples with answers is reached, skipping samples with empty references."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3b8db3a1ad908b0"
  },
  {
   "cell_type": "code",
   "source": [
    "num_valid_examples = config[\"num_valid_examples\"]\n",
    "use_llm = config[\"use_llm\"]\n",
    "\n",
    "results = []\n",
    "i = 0\n",
    "\n",
    "with tqdm(total=num_valid_examples) as pbar:\n",
    "    while len(results) < num_valid_examples and i < len(dataset):\n",
    "        sample = dataset[i]\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        result = process_sample(i, sample, use_llm)\n",
    "        elapsed_time = time.perf_counter() - start_time\n",
    "            \n",
    "        if result:\n",
    "            result[\"elapsed_time\"] = elapsed_time\n",
    "            results.append(result)\n",
    "            pbar.update(1)\n",
    "            \n",
    "        i += 1\n",
    "\n",
    "print(f\"Processed {len(results)} valid samples out of {i} total samples\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "end_time": "2025-06-23T16:29:11.663014Z",
     "start_time": "2025-06-23T16:28:52.883079Z"
    }
   },
   "id": "c273ca2913c053a2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:18<00:00,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 valid samples out of 40 total samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save and display results\n",
    "\n",
    "Print average metric scores, save data to CSV and JSON."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f2581af0cde6a2c"
  },
  {
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "end_time": "2025-06-23T16:29:11.669849Z",
     "start_time": "2025-06-23T16:29:11.664037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for r in results:\n",
    "    print(f\"\\n\\n===================================== SAMPLE {r['index']} =====================================\")\n",
    "    print(f\"QUERY: {r['query']}\\n\")\n",
    "    print(f\"PREDICTION: {r['prediction']}\\n\")\n",
    "    print(f\"PREDICTION (GOLDEN): {r['prediction_golden']}\\n\")\n",
    "    print(f\"GOLDEN ANSWER: {r['gold_answer']}\\n\")\n",
    "    print(f\"rougeL: {r['rougeL']}, rougeL_golden: {r['rougeL_golden']}\")\n",
    "    print(f\"bleu: {r['bleu']}, blue golden: {r['bleu_golden']}\")\n",
    "    print(f\"longest match: {r['longest_match']}, longest match golden: {r['longest_match_golden']}\")"
   ],
   "id": "34638e2b890f8ac6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===================================== SAMPLE 11 =====================================\n",
      "QUERY: when did now thats what i call music come out\n",
      "\n",
      "PREDICTION: Now That's What I Call Music was released on 28 November 1983 in the United Kingdom.\n",
      "\n",
      "PREDICTION (GOLDEN): Now That's What I Call Music, also known as Now or Now 1, was originally released on 28 November 1983.\n",
      "\n",
      "GOLDEN ANSWER: 28 November 1983\n",
      "\n",
      "rougeL: 0.20134228187919462, rougeL_golden: 0.2352941176470588\n",
      "bleu: 0.0002931640620544164, blue golden: 0.0022084718814988106\n",
      "longest match: 0.02972972972972973, longest match golden: 0.02702702702702703\n",
      "\n",
      "\n",
      "===================================== SAMPLE 13 =====================================\n",
      "QUERY: where is the 2018 grey cup being played\n",
      "\n",
      "PREDICTION: The 2018 Grey Cup is being played at The Brick Field at Commonwealth Stadium in Edmonton, Alberta.\n",
      "\n",
      "PREDICTION (GOLDEN): The 2018 Grey Cup is being played at The Brick Field at Commonwealth Stadium in Edmonton, Alberta on Sunday, November 25, 2018.\n",
      "\n",
      "GOLDEN ANSWER: The Brick Field at Commonwealth Stadium in Edmonton , Alberta\n",
      "\n",
      "rougeL: 0.6666666666666667, rougeL_golden: 0.8181818181818182\n",
      "bleu: 0.44441305848035617, blue golden: 0.7727007227608828\n",
      "longest match: 0.4696969696969697, longest match golden: 0.4696969696969697\n",
      "\n",
      "\n",
      "===================================== SAMPLE 16 =====================================\n",
      "QUERY: where did the band bastille get their name\n",
      "\n",
      "PREDICTION: The context does not contain specific information on how the band Bastille got their name.\n",
      "\n",
      "PREDICTION (GOLDEN): The band Bastille got their name from Bastille Day, which is celebrated on 14 July, the date of lead vocalist Dan Smith's birthday.\n",
      "\n",
      "GOLDEN ANSWER: Bastille Day\n",
      "\n",
      "rougeL: 0.10256410256410255, rougeL_golden: 0.3908045977011494\n",
      "bleu: 0.0, blue golden: 0.10549106944966119\n",
      "longest match: 0.02680965147453083, longest match golden: 0.08579088471849866\n",
      "\n",
      "\n",
      "===================================== SAMPLE 17 =====================================\n",
      "QUERY: where is arkansas river located on a map\n",
      "\n",
      "PREDICTION: The Arkansas River is located in the U.S. states of Colorado, Kansas, Oklahoma, and Arkansas. It also drains parts of Texas, New Mexico, and Missouri. It flows generally east and southeast, originating in the Arkansas River Valley in Colorado, through the Rockies, then into the Midwest via Kansas, and finally into the Mississippi River in eastern Arkansas.\n",
      "\n",
      "PREDICTION (GOLDEN): The Arkansas River is located in the United States, flowing through Colorado, Kansas, Oklahoma, and Arkansas, specifically in the Great Plains region. It also drains parts of Texas, New Mexico, and Missouri. The lower part of the river near Little Rock, Arkansas, is mentioned, with specific coordinates provided for locations near Leadville, Colorado, and Napoleon, Arkansas. The river's length is stated as 1,469 mi (2,364 km), and basin size is given as 168,002 sq mi (435,123 km). Discharge measurements for Dardanelle, Arkansas, are also provided. The context does not provide a specific map location beyond these details. If a more precise map location is needed, that information is not contained in the given context.\n",
      "\n",
      "GOLDEN ANSWER: The Arkansas River flows through Colorado , Kansas , Oklahoma , and Arkansas , and its watershed also drains parts of Texas , New Mexico and Missouri .\n",
      "\n",
      "rougeL: 0.12337662337662338, rougeL_golden: 0.22764227642276424\n",
      "bleu: 0.0063601748131300396, blue golden: 0.07550586957986626\n",
      "longest match: 0.017252396166134186, longest match golden: 0.017252396166134186\n",
      "\n",
      "\n",
      "===================================== SAMPLE 18 =====================================\n",
      "QUERY: who plays nicholas in the princess diaries 2\n",
      "\n",
      "PREDICTION: John Rhys-Davies plays Viscount Mabrey in The Princess Diaries 2: Royal Engagement.\n",
      "\n",
      "PREDICTION (GOLDEN): Chris Pine plays Lord Nicholas Devereaux in Princess Diaries 2.\n",
      "\n",
      "GOLDEN ANSWER: Chris Pine\n",
      "\n",
      "rougeL: 0.09375, rougeL_golden: 0.09836065573770492\n",
      "bleu: 0.0, blue golden: 0.0\n",
      "longest match: 0.047619047619047616, longest match golden: 0.0700280112044818\n",
      "\n",
      "\n",
      "===================================== SAMPLE 22 =====================================\n",
      "QUERY: the p-wave phase of an electrocardiogram (ecg) represents\n",
      "\n",
      "PREDICTION: The context contains information that:\n",
      "\n",
      "1. The P wave represents atrial depolarization, resulting in atrial contraction (systole).\n",
      "2. Ventricular rate can affect P wave interpretation, with fast rates possibly causing fibrillatory or flutter waves to be mistaken for P waves.\n",
      "3. P wave absence can indicate fine atrial fibrillation or sinoatrial arrest.\n",
      "4. P waves may not be clearly delineated in surface ECG and may need visualization through a Lewis lead.\n",
      "5. Atrial repolarization, which occurs after the P wave, may cause a Ta wave in the ECG with potential for ST depression, often mistaken for ischemic changes.\n",
      "6. Peaked P waves can indicate right atrial enlargement and cor pulmonale, although with low predictive value.\n",
      "7. AP waves with increased amplitude may suggest hypokalemia or right atrial enlargement, while decreased amplitude may indicate hyperkalemia.\n",
      "8. Bifid P waves (P mitrale) can indicate abnormalities in the left atrium, such as dilatation or hypertrophy.\n",
      "\n",
      "The context does not mention anything about the P wave being uniquely identified by a specific letter or number. Thus, the content in the context doesn't contain that specific information. The answer is I don't know.\n",
      "\n",
      "PREDICTION (GOLDEN): The context contains that the P wave phase of an electrocardiogram (ECG) represents atrial depolarization, which in turn results in atrial contraction, also known as atrial systole.\n",
      "\n",
      "GOLDEN ANSWER: atrial depolarization\n",
      "\n",
      "rougeL: 0.10628019323671498, rougeL_golden: 0.6363636363636364\n",
      "bleu: 0.032240590857028885, blue golden: 0.28695800751729983\n",
      "longest match: 0.2894736842105263, longest match golden: 0.2894736842105263\n",
      "\n",
      "\n",
      "===================================== SAMPLE 23 =====================================\n",
      "QUERY: when did star trek the next generation first air\n",
      "\n",
      "PREDICTION: Star Trek: The Next Generation first aired in its seventh season in December 2002.\n",
      "\n",
      "PREDICTION (GOLDEN): The context contains that \"Star Trek: The Next Generation\" originally aired on September 25, 1989, and the last aired episode mentioned is May 23, 1994. Therefore, the first air date based on the provided context is September 25, 1989.\n",
      "\n",
      "GOLDEN ANSWER: September 28 , 1987\n",
      "\n",
      "rougeL: 0.034482758620689655, rougeL_golden: 0.11347517730496454\n",
      "bleu: 0.0, blue golden: 0.009361295825372052\n",
      "longest match: 0.01864406779661017, longest match golden: 0.02711864406779661\n",
      "\n",
      "\n",
      "===================================== SAMPLE 32 =====================================\n",
      "QUERY: who sang the most wonderful summer of my life\n",
      "\n",
      "PREDICTION: The context reveals that the singer who sang \"The Most Wonderful Summer of My Life\" is Jackie Ward, who is better known as Robin Ward. The context also mentions that she recorded the demo for \"Wonderful Summer\" with songwriter-producer Perry Botkin Jr. and reached #14 on the Billboard Hot 100 chart with this song in 1963.\n",
      "\n",
      "PREDICTION (GOLDEN): Robin Ward sang the song \"Most Wonderful Summer of My Life.\"\n",
      "\n",
      "GOLDEN ANSWER: Jackie Ward\n",
      "\n",
      "rougeL: 0.16923076923076924, rougeL_golden: 0.14285714285714285\n",
      "bleu: 0.05480048702814323, blue golden: 0.0\n",
      "longest match: 0.05708245243128964, longest match golden: 0.03805496828752643\n",
      "\n",
      "\n",
      "===================================== SAMPLE 37 =====================================\n",
      "QUERY: how many oscars did on golden pond win\n",
      "\n",
      "PREDICTION: On Golden Pond won one competitive Oscar, which Henry Fonda won at the age of 76, making him the oldest Oscar winner in that category. The film also earned nominations for five major Academy Awards: Best Picture, Director, Actor, Actress, and Screenplay.\n",
      "\n",
      "PREDICTION (GOLDEN): On Golden Pond won three Oscars: Best Actor, Best Actress, and Best Adapted Screenplay.\n",
      "\n",
      "GOLDEN ANSWER: three\n",
      "\n",
      "rougeL: 0.41428571428571426, rougeL_golden: 0.17857142857142858\n",
      "bleu: 0.13297369770487363, blue golden: 0.0014375120312929335\n",
      "longest match: 0.06818181818181818, longest match golden: 0.045454545454545456\n",
      "\n",
      "\n",
      "===================================== SAMPLE 39 =====================================\n",
      "QUERY: who won season 4 of america's got talent\n",
      "\n",
      "PREDICTION: Kevin Skinner won season 4 of America's Got Talent.\n",
      "\n",
      "PREDICTION (GOLDEN): Kevin Skinner won season 4 of America's Got Talent.\n",
      "\n",
      "GOLDEN ANSWER: Kevin Skinner\n",
      "\n",
      "rougeL: 0.20689655172413793, rougeL_golden: 0.20689655172413793\n",
      "bleu: 0.0, blue golden: 0.0\n",
      "longest match: 0.05102040816326531, longest match golden: 0.05102040816326531\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "source": [
    "output_folder = config[\"output_folder\"]\n",
    "run_name = f\"run_custom_{datetime.now().strftime(\"%Y_%m_%d_%H%M%S\")}\"\n",
    "out_path = os.path.join(output_folder, run_name)\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "response = [\n",
    "    {\n",
    "        \"index\": row[\"index\"],\n",
    "        \"query\": row[\"query\"],\n",
    "        \"gold_answer\": row[\"gold_answer\"],\n",
    "        \"prediction\": row[\"prediction\"],\n",
    "        \"prediction_golden\": row[\"prediction_golden\"],\n",
    "        \"elapsed_time\": row[\"elapsed_time\"]\n",
    "    }\n",
    "    for _, row in results_df.iterrows()\n",
    "]\n",
    "results_df.drop([\"prediction_golden\", \"prediction\", \"query\", \"gold_answer\", \"elapsed_time\"], axis=1, inplace=True)\n",
    "\n",
    "file_path = os.path.join(out_path, f\"results.csv\")\n",
    "results_df.to_csv(file_path, index=False)\n",
    "\n",
    "with open(os.path.join(out_path, \"responses.json\"), \"w\") as f:\n",
    "    json.dump(response, f, indent=4)\n",
    "    \n",
    "with open(os.path.join(out_path, \"used_config.json\"), \"w\") as f:\n",
    "    json.dump(config, f, indent=4)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "end_time": "2025-06-23T16:29:11.683047Z",
     "start_time": "2025-06-23T16:29:11.670877Z"
    }
   },
   "id": "258f9d60c1895cf1",
   "outputs": [],
   "execution_count": 68
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
