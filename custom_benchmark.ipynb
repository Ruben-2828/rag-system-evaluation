{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57f51bb443694561",
   "metadata": {},
   "source": [
    "# RAG Benchmark Evaluation on Natural Questions\n",
    "\n",
    "This notebook evaluates a RAG (Retrieval-Augmented Generation) system on the Natural Questions dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea118ea9e434089f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Setup and Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60407f4d1af3bd96",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Imports and dependencies\n",
    "\n",
    "Import all necessary libraries for dataset loading, embeddings, LLM, text processing, and evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4eea478c45fbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import evaluate\n",
    "\n",
    "from datetime import datetime\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.document import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841ce3ea6db11221",
   "metadata": {},
   "source": [
    "### Config File\n",
    "\n",
    "Configure the dataset split, output folder, embedding model, chunk size, overlap, and LLM parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf0ebd2fe01be71",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"configs/custom_benchmark_config.json\"\n",
    "\n",
    "with open(config_file, \"r\") as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872e3764aa05cf93",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load API Key\n",
    "\n",
    "Load the Hugging Face API key from environment variables to authenticate model requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e3df368ffe7436",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "HF_API_KEY = os.getenv(\"API_KEY4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d918d513f3e953",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load dataset\n",
    "\n",
    "Load the Natural Questions dataset subset for benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292d18be6927d28",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"natural_questions\", split=config[\"dataset_split\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8a0c45ab8fd77c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Model Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ec0c6bbede92a4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Embeddings initialization\n",
    "\n",
    "Create the embedding model for converting text chunks into vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733553b79138a2ce",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=config[\"embedding_model_name\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8989ddb91f0dfa0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Text splitter configuration\n",
    "\n",
    "Define how the document is split into overlapping chunks for embedding and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ec7696c36ded36",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=config[\"chunk_size\"],\n",
    "    chunk_overlap=config[\"chunk_overlap\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75658f377b18de7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### LLM setup\n",
    "\n",
    "Configure the language model endpoint on Hugging Face Hub for text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604630da582b0af3",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=config[\"llm_model\"],\n",
    "    huggingfacehub_api_token=HF_API_KEY,\n",
    "    task=\"text-generation\",\n",
    "    temperature=config[\"temperature\"],\n",
    "    max_new_tokens=config[\"max_new_tokens\"],\n",
    ")\n",
    "chat_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9983a91bdc30f78",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Retrieval and querying\n",
    "\n",
    "Functions to retrieve relevant documents locally and query the LLM with that context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b6e3f8315f79b6",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def retrieve_local(query, vectorstore, k=config[\"top_k\"]):\n",
    "    docs_faiss = vectorstore.similarity_search(query, k=k)\n",
    "    return [(i, d.page_content) for i, d in enumerate(docs_faiss)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f3fb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_doc_ids(split_docs, golden_answer=None):\n",
    "    relevant_ids = []\n",
    "    for doc in split_docs:\n",
    "        content = doc.page_content.lower()\n",
    "        if golden_answer and golden_answer.strip() and golden_answer.strip().lower() in content:\n",
    "            relevant_ids.append(doc.metadata[\"doc_id\"])\n",
    "    return relevant_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d2661109411a70",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def ask(query, context):\n",
    "    messages = [\n",
    "        SystemMessage(content=f\"Just answer queries based on {context}.\"),\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        Answer the query: {query} based uniquely on the context: {context}, don't make up anything, just say what the context contains. If the information is not in the context, you must say you don't know. You must answer only the specified question and nothing else.\n",
    "        \"\"\")\n",
    "    ]\n",
    "    response = chat_model.invoke(messages)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9216a27f27bb44",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data extraction and preprocessing\n",
    "\n",
    "Functions to extract valid answers from dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd55c4e0a2d91e1",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def extract_answers(sample):\n",
    "    tokens = sample[\"document\"][\"tokens\"]\n",
    "    short_answer = \"\"\n",
    "    start = sample[\"annotations\"][\"short_answers\"][0][\"start_token\"]\n",
    "    end = sample[\"annotations\"][\"short_answers\"][0][\"end_token\"]\n",
    "    if len(start) > 0:\n",
    "        short_answer = \" \".join([\n",
    "            t for t, html in zip(tokens[\"token\"][int(start[0]):int(end[0])], tokens[\"is_html\"][start[0]:end[0]]) if not html\n",
    "        ])\n",
    "\n",
    "    long_answer = \"\"\n",
    "    if sample[\"annotations\"][\"long_answer\"][0][\"start_token\"] != -1:\n",
    "        start = sample[\"annotations\"][\"long_answer\"][0][\"start_token\"]\n",
    "        end = sample[\"annotations\"][\"long_answer\"][0][\"end_token\"]\n",
    "        long_answer = \" \".join([\n",
    "            t for t, html in zip(tokens[\"token\"][start:end], tokens[\"is_html\"][start:end]) if not html\n",
    "        ])\n",
    "\n",
    "    return long_answer or \"\", short_answer or \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fa481a60693a36",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def preprocess_text(sample):\n",
    "    tokens = sample[\"document\"][\"tokens\"]\n",
    "    return \" \".join([t for t, html in zip(tokens[\"token\"], tokens[\"is_html\"]) if not html])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b739fec4387fcc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Benchmarking "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5901ab8d628195c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Retrieve benchmark\n",
    "\n",
    "By finding the longest match between the prediction and the golden context, we can evaluate how well the model retrieves relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2c257fcdb1d26b",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def find_longest_match(str1, str2):\n",
    "    max_len = 0\n",
    "    len2 = len(str2)\n",
    "\n",
    "    for i in range(len2):\n",
    "        for j in range(i + 1, len2 + 1):\n",
    "            substr = str2[i:j]\n",
    "            if substr in str1 and len(substr) > max_len:\n",
    "                max_len = len(substr)\n",
    "\n",
    "    return max_len / len2 if len2 > 0 else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5dc984b0153086",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Evaluation benchmark\n",
    "\n",
    "Load rouge and bleu metrics for evaluating the model's predictions against the golden answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76c5209da15957e",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "metric_rouge = evaluate.load(\"rouge\")\n",
    "metric_bleu = evaluate.load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23f2a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def precision_at_k(retrieved, relevant, k):\n",
    "    retrieved_k = retrieved[:k]\n",
    "    return len(set(retrieved_k) & set(relevant)) / k\n",
    "\n",
    "def recall_at_k(retrieved, relevant, k):\n",
    "    retrieved_k = retrieved[:k]\n",
    "    return len(set(retrieved_k) & set(relevant)) / len(relevant) if relevant else 0\n",
    "\n",
    "def average_precision(retrieved, relevant, k):\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "    for i, doc_id in enumerate(retrieved[:k]):\n",
    "        if doc_id in relevant:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "    return score / len(relevant) if relevant else 0\n",
    "\n",
    "def dcg_at_k(retrieved, relevant, k):\n",
    "    dcg = 0.0\n",
    "    for i, doc_id in enumerate(retrieved[:k]):\n",
    "        if doc_id in relevant:\n",
    "            dcg += 1.0 / np.log2(i + 2)\n",
    "    return dcg\n",
    "\n",
    "def ndcg_at_k(retrieved, relevant, k):\n",
    "    dcg = dcg_at_k(retrieved, relevant, k)\n",
    "    idcg = sum(1.0 / np.log2(i + 2) for i in range(min(len(relevant), k)))\n",
    "    return dcg / idcg if idcg > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72a8cb8659c09c9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Single sample processing\n",
    "\n",
    "Process one sample: build context, eventually query LLM, extract answer, and compute metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dd8c46c6a1cb9e",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def process_sample(i, sample):\n",
    "    k_values = config.get(\"k_values\", [1, 3, 5, 10])\n",
    "    golden_context, golden_answer = extract_answers(sample)\n",
    "    if golden_answer == \"\" or golden_context == \"\":\n",
    "        return None\n",
    "\n",
    "    query = sample[\"question\"][\"text\"]\n",
    "    text = preprocess_text(sample)\n",
    "    docs = [Document(page_content=text)]\n",
    "    split_docs = splitter.split_documents(docs)\n",
    "    for idx, doc in enumerate(split_docs):\n",
    "        doc.metadata = {\"doc_id\": f\"doc_{idx}\"}\n",
    "    vectorstore = FAISS.from_documents(split_docs, embedding_model)\n",
    "\n",
    "    docs_faiss = vectorstore.similarity_search(query, k=max(k_values))\n",
    "    retrieved_ids = [d.metadata[\"doc_id\"] for d in docs_faiss]\n",
    "\n",
    "    relevant_ids = get_relevant_doc_ids(split_docs, golden_answer)\n",
    "\n",
    "    precision_dict = {}\n",
    "    recall_dict = {}\n",
    "    map_dict = {}\n",
    "    ndcg_dict = {}\n",
    "    for k in k_values:\n",
    "        precision_dict[f\"precision@{k}\"] = precision_at_k(retrieved_ids, relevant_ids, k)\n",
    "        recall_dict[f\"recall@{k}\"] = recall_at_k(retrieved_ids, relevant_ids, k)\n",
    "        map_dict[f\"map@{k}\"] = average_precision(retrieved_ids, relevant_ids, k)\n",
    "        ndcg_dict[f\"ndcg@{k}\"] = ndcg_at_k(retrieved_ids, relevant_ids, k)\n",
    "\n",
    "    context = [d.page_content for d in docs_faiss]\n",
    "    prediction = ask(query, context)\n",
    "    rouge = metric_rouge.compute(predictions=[prediction], references=[golden_context])\n",
    "    bleu = metric_bleu.compute(predictions=[prediction], references=[golden_context])\n",
    "    longest_match = find_longest_match(prediction, golden_context)\n",
    "    prediction_golden = ask(query, golden_context)\n",
    "    rouge_golden = metric_rouge.compute(predictions=[prediction_golden], references=[golden_context])\n",
    "    bleu_golden = metric_bleu.compute(predictions=[prediction_golden], references=[golden_context])\n",
    "    longest_match_golden = find_longest_match(prediction_golden, golden_context)\n",
    "\n",
    "    result = {\n",
    "        \"index\": i,\n",
    "        \"rougeL\": rouge[\"rougeL\"],\n",
    "        \"rougeL_golden\": rouge_golden[\"rougeL\"],\n",
    "        \"bleu\": bleu[\"bleu\"],\n",
    "        \"bleu_golden\": bleu_golden[\"bleu\"],\n",
    "        \"longest_match\": longest_match,\n",
    "        \"longest_match_golden\": longest_match_golden,\n",
    "        \"query\": query,\n",
    "        \"gold_answer\": golden_answer,\n",
    "        \"prediction\": prediction,\n",
    "        \"prediction_golden\": prediction_golden,\n",
    "        \"retrieved_docs\": context,\n",
    "        \"relevant_ids\": relevant_ids,\n",
    "        \"retrieved_ids\": retrieved_ids,\n",
    "        **precision_dict,\n",
    "        **recall_dict,\n",
    "        **map_dict,\n",
    "        **ndcg_dict\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b8db3a1ad908b0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Full benchmark loop\n",
    "\n",
    "Run the evaluation until the target number of valid samples with answers is reached, skipping samples with empty references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c273ca2913c053a2",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "num_valid_examples = config[\"num_valid_examples\"]\n",
    "\n",
    "results = []\n",
    "i = 0\n",
    "\n",
    "with tqdm(total=num_valid_examples) as pbar:\n",
    "    while len(results) < num_valid_examples and i < len(dataset):\n",
    "        sample = dataset[i]\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        result = process_sample(i, sample)\n",
    "        elapsed_time = time.perf_counter() - start_time\n",
    "            \n",
    "        if result:\n",
    "            result[\"elapsed_time\"] = elapsed_time\n",
    "            results.append(result)\n",
    "            pbar.update(1)\n",
    "            \n",
    "        i += 1\n",
    "\n",
    "print(f\"Processed {len(results)} valid samples out of {i} total samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2581af0cde6a2c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Save and display results\n",
    "\n",
    "Print average metric scores, save data to CSV and JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ac12b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in results:\n",
    "    print(f\"\\n\\n===================================== SAMPLE {r['index']} =====================================\")\n",
    "    print(f\"QUERY: {r['query']}\\n\")\n",
    "    print(f\"PREDICTION: {r['prediction']}\\n\")\n",
    "    print(f\"PREDICTION (GOLDEN): {r['prediction_golden']}\\n\")\n",
    "    print(f\"GOLDEN ANSWER: {r['gold_answer']}\\n\")\n",
    "    print(f\"rougeL: {r['rougeL']}, rougeL_golden: {r['rougeL_golden']}\")\n",
    "    print(f\"bleu: {r['bleu']}, blue golden: {r['bleu_golden']}\")\n",
    "    print(f\"longest match: {r['longest_match']}, longest match golden: {r['longest_match_golden']}\")\n",
    "    print(\"retrieved_ids:\", r['retrieved_ids'])\n",
    "    print(\"relevant_ids:\", r['relevant_ids'])\n",
    "    k_values = config.get(\"k_values\")\n",
    "    for k in k_values:\n",
    "        print(f\"precision@{k}: {r.get(f'precision@{k}', 'NA')}, recall@{k}: {r.get(f'recall@{k}', 'NA')}, map@{k}: {r.get(f'map@{k}', 'NA')}, ndcg@{k}: {r.get(f'ndcg@{k}', 'NA')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258f9d60c1895cf1",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "output_folder = config[\"output_folder\"]\n",
    "run_name = f\"run_custom_{datetime.now().strftime('%Y_%m_%d_%H%M%S')}\"\n",
    "out_path = os.path.join(output_folder, run_name)\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "response = []\n",
    "for _, row in results_df.iterrows():\n",
    "    item = {\n",
    "        \"index\": row[\"index\"],\n",
    "        \"query\": row[\"query\"],\n",
    "        \"gold_answer\": row[\"gold_answer\"],\n",
    "        \"prediction\": row[\"prediction\"],\n",
    "        \"prediction_golden\": row[\"prediction_golden\"],\n",
    "        \"elapsed_time\": row[\"elapsed_time\"],\n",
    "        \"retrieved_docs\": row[\"retrieved_docs\"],\n",
    "        \"relevant_ids\": row[\"relevant_ids\"],\n",
    "        \"retrieved_ids\": row[\"retrieved_ids\"]\n",
    "    }\n",
    "    for k in config[\"k_values\"]:\n",
    "        for metric in [\"precision\", \"recall\", \"map\", \"ndcg\"]:\n",
    "            key = f\"{metric}@{k}\"\n",
    "            if key in row:\n",
    "                item[key] = row[key]\n",
    "    response.append(item)\n",
    "\n",
    "with open(os.path.join(out_path, \"responses.json\"), \"w\") as f:\n",
    "    json.dump(response, f, indent=4)\n",
    "\n",
    "with open(os.path.join(out_path, \"used_config.json\"), \"w\") as f:\n",
    "    json.dump(config, f, indent=4)\n",
    "\n",
    "results_df.drop(\n",
    "    [\"prediction_golden\", \"prediction\", \"query\", \"gold_answer\", \"elapsed_time\", \"retrieved_docs\", \"relevant_ids\", \"retrieved_ids\"],\n",
    "    axis=1, inplace=True\n",
    ")\n",
    "file_path = os.path.join(out_path, f\"results.csv\")\n",
    "results_df.to_csv(file_path, index=False)\n",
    "\n",
    "metrics = [\"precision\", \"recall\", \"map\", \"ndcg\"]\n",
    "k_values = config[\"k_values\"]\n",
    "\n",
    "rows = []\n",
    "for k in k_values:\n",
    "    row = {\"k\": k}\n",
    "    for metric in metrics:\n",
    "        key = f\"{metric}@{k}\"\n",
    "        if key in results_df:\n",
    "            row[metric] = results_df[key].mean()\n",
    "        else:\n",
    "            row[metric] = float('nan')\n",
    "    rows.append(row)\n",
    "\n",
    "metrics_by_k_df = pd.DataFrame(rows)\n",
    "metrics_by_k_df.to_csv(os.path.join(out_path, \"metrics_by_k.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
