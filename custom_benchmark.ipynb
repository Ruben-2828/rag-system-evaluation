{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57f51bb443694561",
   "metadata": {},
   "source": [
    "# RAG Benchmark Evaluation on Natural Questions\n",
    "\n",
    "This notebook evaluates a RAG (Retrieval-Augmented Generation) system on the Natural Questions dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea118ea9e434089f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Setup and Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60407f4d1af3bd96",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Imports and dependencies\n",
    "\n",
    "Import all necessary libraries for dataset loading, embeddings, LLM, text processing, and evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1438,
   "id": "ea4eea478c45fbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.document import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from collections import defaultdict\n",
    "from configs.prompt_templates import get_prompt_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841ce3ea6db11221",
   "metadata": {},
   "source": [
    "### Config File\n",
    "\n",
    "Configure the dataset split, output folder, embedding model, chunk size, overlap, and LLM parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf0ebd2fe01be71",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"configs/custom_benchmark_config.json\"\n",
    "\n",
    "with open(config_file, \"r\") as f:\n",
    "    config = json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4a8bcb",
   "metadata": {},
   "source": [
    "### Only Retrieve Functionality\n",
    " \n",
    "The \"only_retrieve\" functionality allows you to run the benchmark in retrieval-only mode.\n",
    "When enabled, the system will retrieve relevant documents for each query but will not generate answers using an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce000697",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "only_retrieve = config[\"only_retrieve\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872e3764aa05cf93",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load API Key\n",
    "\n",
    "Load the Hugging Face API key from environment variables to authenticate model requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1440,
   "id": "90e3df368ffe7436",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T16:33:00.567283Z",
     "start_time": "2025-07-02T16:33:00.561242Z"
    }
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "HF_API_KEY = os.getenv(\"API_KEY19\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d918d513f3e953",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load dataset\n",
    "\n",
    "Load the Natural Questions dataset subset for benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1441,
   "id": "292d18be6927d28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T16:33:05.613036Z",
     "start_time": "2025-07-02T16:33:00.568727Z"
    },
    "is_executing": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e1099627aa64307813ef48cd7f149d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/287 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63415ceadb9c4d4ebc7c4e978fc4ade4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/287 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"natural_questions\", split=config[\"dataset_split\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8a0c45ab8fd77c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Model Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ec0c6bbede92a4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Embeddings initialization\n",
    "\n",
    "Create the embedding model for converting text chunks into vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1442,
   "id": "733553b79138a2ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T16:33:07.494706Z",
     "start_time": "2025-07-02T16:33:05.614373Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=config[\"embedding_model_name\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8989ddb91f0dfa0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Text splitter configuration\n",
    "\n",
    "Define how the document is split into overlapping chunks for embedding and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1443,
   "id": "c4ec7696c36ded36",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=config[\"chunk_size\"],\n",
    "    chunk_overlap=config[\"chunk_overlap\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75658f377b18de7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### LLM setup\n",
    "\n",
    "Configure the language model endpoint on Hugging Face Hub for text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1444,
   "id": "604630da582b0af3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T16:33:07.505915Z",
     "start_time": "2025-07-02T16:33:07.501012Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "if not only_retrieve:\n",
    "    llm = HuggingFaceEndpoint(\n",
    "        repo_id=config[\"llm_model\"],\n",
    "        huggingfacehub_api_token=HF_API_KEY,\n",
    "        task=\"text-generation\",\n",
    "        temperature=config[\"temperature\"],\n",
    "        max_new_tokens=config[\"max_new_tokens\"],\n",
    "    )\n",
    "    chat_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9983a91bdc30f78",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Retrieval and querying\n",
    "\n",
    "Functions to retrieve relevant documents locally and query the LLM with that context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20b6e3f8315f79b6",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mretrieve_local\u001b[39m(query, vectorstore, k=\u001b[43mconfig\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mtop_k\u001b[39m\u001b[33m\"\u001b[39m]):\n\u001b[32m      2\u001b[39m     docs_faiss = vectorstore.similarity_search(query, k=k)\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [d.page_content \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m docs_faiss]\n",
      "\u001b[31mNameError\u001b[39m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "def retrieve_local(query, vectorstore, k=config[\"top_k\"]):\n",
    "    docs_faiss = vectorstore.similarity_search(query, k=k)\n",
    "    return [d.page_content for d in docs_faiss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f3fb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_doc_ids(split_docs, golden_answer=None):\n",
    "    relevant_ids = []\n",
    "    for doc in split_docs:\n",
    "        content = doc.page_content.lower()\n",
    "        if golden_answer and golden_answer.strip() and golden_answer.strip().lower() in content:\n",
    "            relevant_ids.append(doc.metadata[\"doc_id\"])\n",
    "    return relevant_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d2661109411a70",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def ask(query, context):\n",
    "    messages = get_prompt_messages(config[\"prompt_type\"], context, query)\n",
    "    response = chat_model.invoke(messages)\n",
    "    cleaned = re.sub(r\"<think>[\\s\\S]*?</think>\\s*\", \"\", response.content, flags=re.DOTALL)\n",
    "    return cleaned.strip()\n",
    "\n",
    "#ask(\"What is the capital of France?\", \"Paris is the capital of france.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf43775b",
   "metadata": {},
   "source": [
    "### Semantic Consistency Check\n",
    "This cell defines a function to verify if a predicted answer is semantically consistent with the ground truth using an LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1448,
   "id": "bb2ebed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_check(query, ground_truth, prediction):\n",
    "    messages = [\n",
    "        #SystemMessage(content=f\"Just answer queries based on {ground_truth}.\"),\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        {ground_truth}\n",
    "\n",
    "        given the previous statements, you MUST say (shortly) if the following phrase is correct\n",
    "\n",
    "        {prediction}\n",
    "        \"\"\")\n",
    "    ]\n",
    "    response = chat_model.invoke(messages)\n",
    "    cleaned = re.sub(r\"\\s*<think>[\\s\\S]*?</think>\\s*\", \"\", response.content, flags=re.DOTALL)\n",
    "    return cleaned.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9216a27f27bb44",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data extraction and preprocessing\n",
    "\n",
    "Functions to extract valid answers from dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1449,
   "id": "1dd55c4e0a2d91e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T16:33:07.525093Z",
     "start_time": "2025-07-02T16:33:07.517256Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def extract_answers(sample):\n",
    "    tokens = sample[\"document\"][\"tokens\"]\n",
    "    short_answer = \"\"\n",
    "    start = sample[\"annotations\"][\"short_answers\"][0][\"start_token\"]\n",
    "    end = sample[\"annotations\"][\"short_answers\"][0][\"end_token\"]\n",
    "    if len(start) > 0:\n",
    "        short_answer = \" \".join([\n",
    "            t for t, html in zip(tokens[\"token\"][int(start[0]):int(end[0])], tokens[\"is_html\"][start[0]:end[0]]) if not html\n",
    "        ])\n",
    "\n",
    "    long_answer = \"\"\n",
    "    if sample[\"annotations\"][\"long_answer\"][0][\"start_token\"] != -1:\n",
    "        start = sample[\"annotations\"][\"long_answer\"][0][\"start_token\"]\n",
    "        end = sample[\"annotations\"][\"long_answer\"][0][\"end_token\"]\n",
    "        long_answer = \" \".join([\n",
    "            t for t, html in zip(tokens[\"token\"][start:end], tokens[\"is_html\"][start:end]) if not html\n",
    "        ])\n",
    "\n",
    "    return long_answer or \"\", short_answer or \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1450,
   "id": "b2fa481a60693a36",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def preprocess_text(sample):\n",
    "    tokens = sample[\"document\"][\"tokens\"]\n",
    "    return \" \".join([t for t, html in zip(tokens[\"token\"], tokens[\"is_html\"]) if not html])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b739fec4387fcc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Benchmarking "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5901ab8d628195c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Retrieve benchmark\n",
    "\n",
    "By finding the longest match between the prediction and the golden context, we can evaluate how well the model retrieves relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1451,
   "id": "5b2c257fcdb1d26b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T16:33:07.538105Z",
     "start_time": "2025-07-02T16:33:07.532475Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def find_longest_match(str1, str2):\n",
    "    max_len = 0\n",
    "    len2 = len(str2)\n",
    "\n",
    "    for i in range(len2):\n",
    "        for j in range(i + 1, len2 + 1):\n",
    "            substr = str2[i:j]\n",
    "            if substr in str1 and len(substr) > max_len:\n",
    "                max_len = len(substr)\n",
    "\n",
    "    return max_len / len2 if len2 > 0 else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5dc984b0153086",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Evaluation benchmark\n",
    "\n",
    "# Load ROUGE, BLEU, and BLEURT metrics, and initialize the LLM comparison model for evaluating predictions against golden answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1452,
   "id": "b76c5209da15957e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T16:33:10.160892Z",
     "start_time": "2025-07-02T16:33:07.540605Z"
    },
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint C:\\Users\\ruben\\.cache\\huggingface\\metrics\\bleurt\\bleurt-large-512\\downloads\\extracted\\b2c310c50992e604abf5e8f9f4695388f5f2e7bfdfc3d5d81a5f4693d2f300d3\\bleurt-large-512.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint C:\\Users\\ruben\\.cache\\huggingface\\metrics\\bleurt\\bleurt-large-512\\downloads\\extracted\\b2c310c50992e604abf5e8f9f4695388f5f2e7bfdfc3d5d81a5f4693d2f300d3\\bleurt-large-512.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint bert_custom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint bert_custom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:bert_custom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:bert_custom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:vocab.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating WordPiece tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating WordPiece tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    }
   ],
   "source": [
    "if not only_retrieve:\n",
    "    metric_rouge = evaluate.load(\"rouge\")\n",
    "    metric_bleu = evaluate.load(\"bleu\")\n",
    "    metric_bleurt = evaluate.load(\"bleurt\", \"bleurt-large-512\")\n",
    "    comparison_model = SentenceTransformer(config[\"embedding_model_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d097b14",
   "metadata": {},
   "source": [
    "### Retrieval Metrics Functions\n",
    " \n",
    "The following code cell defines functions to compute common retrieval metrics such as Precision@k, Recall@k, Average Precision, DCG, and nDCG, which are used to evaluate the quality of retrieved documents against relevant ground truth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1453,
   "id": "b23f2a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(retrieved, relevant, k):\n",
    "    retrieved_k = retrieved[:k]\n",
    "    return len(set(retrieved_k) & set(relevant)) / k\n",
    "\n",
    "def recall_at_k(retrieved, relevant, k):\n",
    "    retrieved_k = retrieved[:k]\n",
    "    return len(set(retrieved_k) & set(relevant)) / len(relevant) if relevant else 0\n",
    "\n",
    "def average_precision(retrieved, relevant, k):\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "    for i, doc_id in enumerate(retrieved[:k]):\n",
    "        if doc_id in relevant:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "    return score / len(relevant) if relevant else 0\n",
    "\n",
    "def dcg_at_k(retrieved, relevant, k):\n",
    "    dcg = 0.0\n",
    "    for i, doc_id in enumerate(retrieved[:k]):\n",
    "        if doc_id in relevant:\n",
    "            dcg += 1.0 / np.log2(i + 2)\n",
    "    return dcg\n",
    "\n",
    "def ndcg_at_k(retrieved, relevant, k):\n",
    "    dcg = dcg_at_k(retrieved, relevant, k)\n",
    "    idcg = sum(1.0 / np.log2(i + 2) for i in range(min(len(relevant), k)))\n",
    "    return dcg / idcg if idcg > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72a8cb8659c09c9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Single sample processing\n",
    "\n",
    "Process one sample: build context, eventually query LLM, extract answer, and compute metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1454,
   "id": "91dd8c46c6a1cb9e",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def process_sample(i, sample):\n",
    "    k_values = config.get(\"k_values\", [1, 3, 5, 10])\n",
    "    golden_context, golden_answer = extract_answers(sample)\n",
    "    if golden_answer == \"\" or golden_context == \"\":\n",
    "        return None\n",
    "    \n",
    "    query = sample[\"question\"][\"text\"]\n",
    "\n",
    "    text = preprocess_text(sample)\n",
    "    docs = [Document(page_content=text)]\n",
    "    split_docs = splitter.split_documents(docs)\n",
    "    for idx, doc in enumerate(split_docs):\n",
    "        doc.metadata = {\"doc_id\": f\"doc_{idx}\"}\n",
    "    vectorstore = FAISS.from_documents(split_docs, embedding_model)\n",
    "\n",
    "    docs_faiss = vectorstore.similarity_search(query, k=config['top_k'])\n",
    "    retrieved_ids = [d.metadata[\"doc_id\"] for d in docs_faiss]\n",
    "\n",
    "    relevant_ids = get_relevant_doc_ids(split_docs, golden_answer)\n",
    "\n",
    "    precision_dict = {}\n",
    "    recall_dict = {}\n",
    "    map_dict = {}\n",
    "    ndcg_dict = {}\n",
    "    for k in k_values:\n",
    "        precision_dict[f\"precision@{k}\"] = precision_at_k(retrieved_ids, relevant_ids, k)\n",
    "        recall_dict[f\"recall@{k}\"] = recall_at_k(retrieved_ids, relevant_ids, k)\n",
    "        map_dict[f\"map@{k}\"] = average_precision(retrieved_ids, relevant_ids, k)\n",
    "        ndcg_dict[f\"ndcg@{k}\"] = ndcg_at_k(retrieved_ids, relevant_ids, k)\n",
    "\n",
    "    if not only_retrieve:\n",
    "        golden_embeddings = comparison_model.encode(golden_context, convert_to_tensor=True)\n",
    "        \n",
    "        context = retrieve_local(query, vectorstore)\n",
    "        prediction = ask(query, context)\n",
    "        \n",
    "        rouge = metric_rouge.compute(predictions=[prediction], references=[golden_context])\n",
    "        bleu = metric_bleu.compute(predictions=[prediction], references=[golden_context])\n",
    "        bleurt = metric_bleurt.compute(predictions=[prediction], references=[golden_context])\n",
    "        longest_match = find_longest_match(prediction, golden_context)\n",
    "        pred_embeddings = comparison_model.encode(prediction, convert_to_tensor=True)\n",
    "        cosine_similarity = util.pytorch_cos_sim(pred_embeddings, golden_embeddings)\n",
    "        semantic_evaluation = semantic_check(query, golden_context, prediction)\n",
    "\n",
    "        prediction_golden = ask(query, golden_context)\n",
    "            \n",
    "        rouge_golden = metric_rouge.compute(predictions=[prediction_golden], references=[golden_context])\n",
    "        bleu_golden = metric_bleu.compute(predictions=[prediction_golden], references=[golden_context])\n",
    "        bleurt_golden = metric_bleurt.compute(predictions=[prediction_golden], references=[golden_context])\n",
    "        longest_match_golden = find_longest_match(prediction_golden, golden_context)\n",
    "        pred_embeddings_golden = comparison_model.encode(prediction_golden, convert_to_tensor=True)\n",
    "        cosine_similarity_golden = util.pytorch_cos_sim(pred_embeddings_golden, golden_embeddings)\n",
    "        semantic_evaluation_golden = semantic_check(query, golden_context, prediction_golden)\n",
    "    \n",
    "        result = {\n",
    "            \"index\": i,\n",
    "            \"rougeL\": rouge[\"rougeL\"],\n",
    "            \"rougeL_golden\": rouge_golden[\"rougeL\"],\n",
    "            \"bleu\": bleu[\"bleu\"],\n",
    "            \"bleu_golden\": bleu_golden[\"bleu\"],\n",
    "            \"bleurt\": bleurt[\"scores\"][0],\n",
    "            \"bleurt_golden\": bleurt_golden[\"scores\"][0],\n",
    "            \"cosine_similarity\": cosine_similarity.item(),\n",
    "            \"cosine_similarity_golden\": cosine_similarity_golden.item(),\n",
    "            \"longest_match\": longest_match,\n",
    "            \"longest_match_golden\": longest_match_golden,\n",
    "            \"query\": query,\n",
    "            \"gold_answer\": golden_answer,\n",
    "            \"golden_context\": golden_context,\n",
    "            \"prediction\": prediction,\n",
    "            \"prediction_golden\": prediction_golden,\n",
    "            \"semantic_evaluation\": semantic_evaluation,\n",
    "            \"semantic_evaluation_golden\": semantic_evaluation_golden,\n",
    "            \"retrieved_docs\": context,\n",
    "            \"relevant_ids\": relevant_ids,\n",
    "            \"retrieved_ids\": retrieved_ids,\n",
    "            \"precision_dict\": precision_dict,\n",
    "            \"recall_dict\": recall_dict,\n",
    "            \"map_dict\": map_dict,\n",
    "            \"ndcg_dict\": ndcg_dict\n",
    "        }\n",
    "    else:\n",
    "        result = {\n",
    "            \"index\": i,\n",
    "            \"query\": query,\n",
    "            \"gold_answer\": golden_answer,\n",
    "            \"golden_context\": golden_context,\n",
    "            \"relevant_ids\": relevant_ids,\n",
    "            \"retrieved_ids\": retrieved_ids,\n",
    "            \"precision_dict\": precision_dict,\n",
    "            \"recall_dict\": recall_dict,\n",
    "            \"map_dict\": map_dict,\n",
    "            \"ndcg_dict\": ndcg_dict\n",
    "        }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b8db3a1ad908b0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Full benchmark loop\n",
    "\n",
    "Run the evaluation until the target number of valid samples with answers is reached, skipping samples with empty references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1455,
   "id": "c273ca2913c053a2",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a0d4c62ccfc4741bbc3d3e5ca3d7108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 valid samples out of 40 total samples\n"
     ]
    }
   ],
   "source": [
    "num_valid_examples = config[\"num_valid_examples\"]\n",
    "\n",
    "results = []\n",
    "i = 0\n",
    "\n",
    "with tqdm(total=num_valid_examples) as pbar:\n",
    "    while len(results) < num_valid_examples and i < len(dataset):\n",
    "        sample = dataset[i]\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        result = process_sample(i, sample)\n",
    "        elapsed_time = time.perf_counter() - start_time\n",
    "            \n",
    "        if result:\n",
    "            result[\"elapsed_time\"] = elapsed_time\n",
    "            results.append(result)\n",
    "            pbar.update(1)\n",
    "            \n",
    "        i += 1\n",
    "\n",
    "print(f\"Processed {len(results)} valid samples out of {i} total samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2581af0cde6a2c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Save and display results\n",
    "\n",
    "Print average metric scores, save data to CSV and JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1456,
   "id": "25ac12b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===================================== SAMPLE 11 =====================================\n",
      "QUERY: when did now thats what i call music come out\n",
      "\n",
      "GROUND TRUTH: Now That 's What I Call Music ( also simply titled Now or Now 1 ) is the first album from the popular Now ! series that was released in the United Kingdom on 28 November 1983 . Initial pressings were released on vinyl and audio cassette . To celebrate the 25th anniversary of the album and series , the album was re-released on CD for the first time in 2009 . However , alternative longer mixes of Only For Love , Double Dutch and Candy Girl were included in place of the original shorter single mixes from 1983 . A double vinyl re-release followed for Record Store Day on 18 April 2015 . In July 2018 , the album was newly remastered and re-released on CD , vinyl and cassette to commemorate the release of the 100th volume of the series .\n",
      "\n",
      "GOLDEN ANSWER: 28 November 1983\n",
      "\n",
      "PREDICTION: The album \"Now That's What I Call Music\" (also titled \"Now\" or \"Now 1\") was released in the United Kingdom on 28 November 1983. The context explicitly states, \"Now That 's What I Call Music ( 1983 ) Now That 's What I Call Music II ( 1984 ) Now That 's What I Call Music ( also simply titled Now or Now 1 ) is the first album from the popular Now ! series that was released in the United Kingdom on 28 November 1983.\" Therefore, the answer is supported by the context and does not involve any assumptions or external knowledge. The confidence level is high as the information is directly provided in the context.\n",
      "\n",
      "PREDICTION SEMANTIC EVALUATION: The statement is correct. The context explicitly provides the release date as \"28 November 1983\" for the album \"Now That's What I Call Music\" (also titled \"Now\" or \"Now 1\") in the United Kingdom. The user's paraphrase accurately reflects this information without introducing errors. \n",
      "\n",
      "**Answer:** Yes, the statement is correct.\n",
      "PREDICTION GOLDEN: Now That's What I Call Music was released in the United Kingdom on 28 November 1983. This information is directly supported by the context provided, which states, \"Now That 's What I Call Music ( also simply titled Now or Now 1 ) is the first album from the popular Now ! series that was released in the United Kingdom on 28 November 1983.\" There are no assumptions made, and the answer directly addresses the question. The context also confirms the release date for the album, which is the basis for the answer. The confidence level is high as the information is directly supported by the context.\n",
      "\n",
      "PREDICTION GOLDEN SEMANTIC EVALUATION: The statement is correct. The context explicitly states that the album was released in the United Kingdom on 28 November 1983.\n",
      "rougeL: 0.3374485596707819, rougeL_golden: 0.38818565400843885\n",
      "bleu: 0.28730422941995265, bleu_golden: 0.2973031236439101\n",
      "bleurt: -0.5852848291397095, bleurt_golden: -0.5625527501106262\n",
      "cosine_similarity: 0.7968349456787109, cosine_similarity_golden: 0.8528814315795898\n",
      "longest match: 0.23513513513513515, longest_match_golden: 0.23513513513513515\n",
      "retrieved_ids: ['doc_2', 'doc_0', 'doc_1', 'doc_17', 'doc_12']\n",
      "relevant_ids: ['doc_1', 'doc_2']\n",
      "precision:  {'precision@1': 1.0, 'precision@3': 0.6666666666666666, 'precision@5': 0.4, 'precision@10': 0.2}\n",
      "recall:  {'recall@1': 0.5, 'recall@3': 1.0, 'recall@5': 1.0, 'recall@10': 1.0}\n",
      "map:  {'map@1': 0.5, 'map@3': 0.8333333333333333, 'map@5': 0.8333333333333333, 'map@10': 0.8333333333333333}\n",
      "ndcg:  {'ndcg@1': np.float64(1.0), 'ndcg@3': np.float64(0.9197207891481876), 'ndcg@5': np.float64(0.9197207891481876), 'ndcg@10': np.float64(0.9197207891481876)}\n",
      "\n",
      "\n",
      "===================================== SAMPLE 13 =====================================\n",
      "QUERY: where is the 2018 grey cup being played\n",
      "\n",
      "GROUND TRUTH: The game is scheduled to be played at The Brick Field at Commonwealth Stadium in Edmonton , Alberta on Sunday , November 25 , 2018 .\n",
      "\n",
      "GOLDEN ANSWER: The Brick Field at Commonwealth Stadium in Edmonton , Alberta\n",
      "\n",
      "PREDICTION: Based on the provided context, the 2018 Grey Cup game is scheduled to be played at The Brick Field at Commonwealth Stadium in Edmonton, Alberta, Canada. This information is confirmed by the statement: \"The game is scheduled to be played at The Brick Field at Commonwealth Stadium in Edmonton, Alberta on Sunday, November 25, 2018.\" The context also mentions that Calgary and Edmonton were bidding to host the game, and Edmonton was officially named the host city on June 5, 2017.\n",
      "\n",
      "PREDICTION SEMANTIC EVALUATION: Yes, the statement is correct. The context confirms the 2018 Grey Cup game is scheduled for November 25, 2018, at The Brick Field at Commonwealth Stadium in Edmonton, Alberta. The location and year are accurately stated, even though the specific date is also provided in the context.\n",
      "PREDICTION GOLDEN: 1. Verification: The context states that \"The game is scheduled to be played at The Brick Field at Commonwealth Stadium in Edmonton, Alberta on Sunday, November 25, 2018.\"\n",
      "2. Assumptions: None were made beyond what the context provided.\n",
      "3. Answer: The 2018 Grey Cup is being played at The Brick Field at Commonwealth Stadium in Edmonton, Alberta.\n",
      "4. External knowledge: None was used.\n",
      "5. Confidence level: High, as the information is directly provided in the context.\n",
      "\n",
      "PREDICTION GOLDEN SEMANTIC EVALUATION: The context explicitly states the game is scheduled for Sunday, November 25, 2018, at The Brick Field at Commonwealth Stadium in Edmonton, Alberta. The answer correctly identifies this as the 2018 Grey Cup, which aligns with the provided details. \n",
      "\n",
      "**Answer:** Yes, the statement is correct.  \n",
      "**Confidence Level:** High.\n",
      "rougeL: 0.4271844660194175, rougeL_golden: 0.4489795918367347\n",
      "bleu: 0.2617776410827886, bleu_golden: 0.25363836589948674\n",
      "bleurt: 0.21244563162326813, bleurt_golden: -0.17914462089538574\n",
      "cosine_similarity: 0.7724071741104126, cosine_similarity_golden: 0.7504553198814392\n",
      "longest match: 0.6742424242424242, longest_match_golden: 0.6742424242424242\n",
      "retrieved_ids: ['doc_1', 'doc_2', 'doc_4', 'doc_6', 'doc_5']\n",
      "relevant_ids: ['doc_1']\n",
      "precision:  {'precision@1': 1.0, 'precision@3': 0.3333333333333333, 'precision@5': 0.2, 'precision@10': 0.1}\n",
      "recall:  {'recall@1': 1.0, 'recall@3': 1.0, 'recall@5': 1.0, 'recall@10': 1.0}\n",
      "map:  {'map@1': 1.0, 'map@3': 1.0, 'map@5': 1.0, 'map@10': 1.0}\n",
      "ndcg:  {'ndcg@1': np.float64(1.0), 'ndcg@3': np.float64(1.0), 'ndcg@5': np.float64(1.0), 'ndcg@10': np.float64(1.0)}\n",
      "\n",
      "\n",
      "===================================== SAMPLE 16 =====================================\n",
      "QUERY: where did the band bastille get their name\n",
      "\n",
      "GROUND TRUTH: Bastille ( stylised as BΔSTILLE ) are a British rock band formed in 2010 . The group began as a solo project by lead vocalist Dan Smith , but later expanded to include keyboardist Kyle Simmons , bassist and guitarist Will Farquarson , and drummer Chris Wood . The name of the band derives from Bastille Day , which is celebrated on 14 July , the date of Smith 's birthday .\n",
      "\n",
      "GOLDEN ANSWER: Bastille Day\n",
      "\n",
      "PREDICTION: The band Bastille got their name from the French word \"Bastille,\" which refers to the Parisian fortress that was stormed during the French Revolution. The band members chose this name because they wanted to evoke a sense of revolution and change, similar to the events that took place during the French Revolution. The name was also chosen to emphasize the band's unique sound and style, which blends elements of indie pop, synth-pop, and indie rock.\n",
      "\n",
      "PREDICTION SEMANTIC EVALUATION: The phrase is not correct based on the provided statements. The original text specifies the band's name originates from Bastille Day and Dan Smith's birthday, but it does not mention the French Revolution or the Bastille fortress. These details are not supported by the given information.\n",
      "PREDICTION GOLDEN: The band Bastille got their name from **Bastille Day**, a French national holiday celebrated on **14 July**, which is also the date of lead vocalist Dan Smith's birthday. This information is explicitly stated in the context.  \n",
      "\n",
      "**Verification**: Supported by the context's last sentence.  \n",
      "**Assumptions**: None made; the origin is directly attributed to Bastille Day and Dan Smith's birthday.  \n",
      "**Answer Directness**: The question is fully addressed by stating the origin as Bastille Day and its connection to Smith's birthday.  \n",
      "**External Knowledge**: No external information used.  \n",
      "**Confidence Level**: High (100% supported by the provided context).\n",
      "\n",
      "PREDICTION GOLDEN SEMANTIC EVALUATION: **Verification**: Supported by the context's last sentence.  \n",
      "**Assumptions**: None made; the origin is directly attributed to Bastille Day and Dan Smith's birthday.  \n",
      "**Answer Directness**: The question is fully addressed by stating the origin as Bastille Day and its connection to Smith's birthday.  \n",
      "**External Knowledge**: No external information used.  \n",
      "**Confidence Level**: High (100% supported by the provided context).  \n",
      "\n",
      "The phrase is correct as it accurately reflects the origin of the band's name and its connection to Dan Smith's birthday.\n",
      "rougeL: 0.14285714285714285, rougeL_golden: 0.2\n",
      "bleu: 0.0, bleu_golden: 0.0\n",
      "bleurt: -0.4707416594028473, bleurt_golden: -0.6909804344177246\n",
      "cosine_similarity: 0.723508894443512, cosine_similarity_golden: 0.8036473989486694\n",
      "longest match: 0.029490616621983913, longest_match_golden: 0.064343163538874\n",
      "retrieved_ids: ['doc_0', 'doc_35', 'doc_8', 'doc_18', 'doc_2']\n",
      "relevant_ids: ['doc_1', 'doc_49']\n",
      "precision:  {'precision@1': 0.0, 'precision@3': 0.0, 'precision@5': 0.0, 'precision@10': 0.0}\n",
      "recall:  {'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0}\n",
      "map:  {'map@1': 0.0, 'map@3': 0.0, 'map@5': 0.0, 'map@10': 0.0}\n",
      "ndcg:  {'ndcg@1': np.float64(0.0), 'ndcg@3': np.float64(0.0), 'ndcg@5': np.float64(0.0), 'ndcg@10': np.float64(0.0)}\n",
      "\n",
      "\n",
      "===================================== SAMPLE 17 =====================================\n",
      "QUERY: where is arkansas river located on a map\n",
      "\n",
      "GROUND TRUTH: Arkansas River River The lower part of the Arkansas River near Little Rock , Arkansas Country United States States Colorado , Kansas , Oklahoma , Arkansas Region Great Plains Part of Mississippi River watershed Tributaries - left Fountain Creek , Pawnee River , Little Arkansas River , Walnut River , Verdigris River , Neosho River - right Cimarron River , Salt Fork Arkansas River , Canadian River , Poteau River Cities Pueblo , CO , Wichita , KS , Tulsa , OK , Muskogee , OK , Fort Smith , AR , Little Rock , AR , Pine Bluff , AR Source Confluence of East Fork Arkansas River and Tennessee Creek - location Near Leadville , Rocky Mountains , Colorado - elevation 9,728 ft ( 2,965 m ) - coordinates 39 ° 15 ′ 30 '' N 106 ° 20 ′ 38 '' W ﻿ / ﻿ 39.25833 ° N 106.34389 ° W ﻿ / 39.25833 ; - 106.34389 Mouth Mississippi River - location Franklin Township , Desha County , near Napoleon , Arkansas - elevation 108 ft ( 33 m ) - coordinates 33 ° 46 ′ 30 '' N 91 ° 04 ′ 15 '' W ﻿ / ﻿ 33.77500 ° N 91.07083 ° W ﻿ / 33.77500 ; - 91.07083 Coordinates : 33 ° 46 ′ 30 '' N 91 ° 04 ′ 15 '' W ﻿ / ﻿ 33.77500 ° N 91.07083 ° W ﻿ / 33.77500 ; - 91.07083 Length 1,469 mi ( 2,364 km ) , West - east Basin 168,002 sq mi ( 435,123 km ) Discharge for Dardanelle , Arkansas , river mile 219.5 ( river kilometer 353.3 ) - average 40,517 cu ft / s ( 1,147 m / s ) - max 683,000 cu ft / s ( 19,340 m / s ) - min 1,207 cu ft / s ( 34 m / s ) The Arkansas River flows through Colorado , Kansas , Oklahoma , and Arkansas , and its watershed also drains parts of Texas , New Mexico and Missouri .\n",
      "\n",
      "GOLDEN ANSWER: The Arkansas River flows through Colorado , Kansas , Oklahoma , and Arkansas , and its watershed also drains parts of Texas , New Mexico and Missouri .\n",
      "\n",
      "PREDICTION: The Arkansas River is located in the western, central, and southeastern regions of the United States, flowing through the states of Colorado, Kansas, Oklahoma, and Arkansas. It originates in the western United States, specifically in the Arkansas River Valley of Colorado, and eventually flows into the Mississippi River. The river's watershed covers parts of Texas, New Mexico, Missouri, and Arkansas as well. On a map, you would typically find the Arkansas River flowing through these states, passing through the Rocky Mountains in Colorado, the Great Plains in Kansas and Oklahoma, and the eastern forests of Arkansas before joining the Mississippi River.\n",
      "\n",
      "PREDICTION SEMANTIC EVALUATION: The statement is correct. The Arkansas River flows through Colorado (western), Kansas (central), Oklahoma (central), and Arkansas (southeastern) as described, originates in the Rocky Mountains of Colorado, and drains parts of Texas, New Mexico, Missouri, and Arkansas, before flowing into the Mississippi River.\n",
      "PREDICTION GOLDEN: The Arkansas River is located on a map in the states of Colorado, Kansas, Oklahoma, and Arkansas, specifically in the Great Plains region. Its watershed drains parts of Texas, New Mexico, and Missouri as well. The river flows from the Rocky Mountains in Colorado to the Mississippi River in Arkansas. The specific coordinates of the mouth of the Arkansas River near Napoleon, Arkansas, are 33° 46′ 30″ N, 91° 04′ 15″ W.\n",
      "\n",
      "PREDICTION GOLDEN SEMANTIC EVALUATION: The statement is correct. The Arkansas River flows through Colorado, Kansas, Oklahoma, and Arkansas, in the Great Plains region. Its watershed includes parts of Texas, New Mexico, and Missouri. The river originates in the Rocky Mountains and flows into the Mississippi River near Napoleon, Arkansas, with the specified coordinates (33° 46′ 30″ N, 91° 04′ 15″ W) accurate for its mouth.\n",
      "rougeL: 0.16477272727272727, rougeL_golden: 0.1863354037267081\n",
      "bleu: 0.025883739219102687, bleu_golden: 0.013917071762180109\n",
      "bleurt: -0.8053521513938904, bleurt_golden: -0.6936509609222412\n",
      "cosine_similarity: 0.8017061352729797, cosine_similarity_golden: 0.8419846296310425\n",
      "longest match: 0.014057507987220448, longest_match_golden: 0.017891373801916934\n",
      "retrieved_ids: ['doc_4', 'doc_3', 'doc_13', 'doc_9', 'doc_46']\n",
      "relevant_ids: ['doc_3']\n",
      "precision:  {'precision@1': 0.0, 'precision@3': 0.3333333333333333, 'precision@5': 0.2, 'precision@10': 0.1}\n",
      "recall:  {'recall@1': 0.0, 'recall@3': 1.0, 'recall@5': 1.0, 'recall@10': 1.0}\n",
      "map:  {'map@1': 0.0, 'map@3': 0.5, 'map@5': 0.5, 'map@10': 0.5}\n",
      "ndcg:  {'ndcg@1': np.float64(0.0), 'ndcg@3': np.float64(0.6309297535714575), 'ndcg@5': np.float64(0.6309297535714575), 'ndcg@10': np.float64(0.6309297535714575)}\n",
      "\n",
      "\n",
      "===================================== SAMPLE 18 =====================================\n",
      "QUERY: who plays nicholas in the princess diaries 2\n",
      "\n",
      "GROUND TRUTH: Most of the cast returned from the first film , including Julie Andrews , Anne Hathaway , Héctor Elizondo , Heather Matarazzo , and Larry Miller . Garry Marshall returned to direct and Debra Martin Chase to produce . New characters include Viscount Mabrey ( John Rhys - Davies ) , Lord Nicholas Devereaux ( Chris Pine ) , and Andrew Jacoby ( Callum Blue ) .\n",
      "\n",
      "GOLDEN ANSWER: Chris Pine\n",
      "\n",
      "PREDICTION: The character Nicholas is played by Chris Pine in The Princess Diaries 2: Royal Engagement. The context states, \"New characters include Viscount Mabrey (John Rhys-Davies), and Garry Marshall directed and Debra Martin Chase produced the film.\" The character Nicholas is identified as Viscount Mabrey, and John Rhys-Davies portrays this character.\n",
      "\n",
      "PREDICTION SEMANTIC EVALUATION: Yes, the statement is correct. The character \"Lord Nicholas Devereaux\" (referred to as \"Nicholas\" in the statement) is played by Chris Pine in *The Princess Diaries 2: Royal Engagement*. The context explicitly names Chris Pine as the actor for this role.\n",
      "PREDICTION GOLDEN: John Rhys-Davies plays Nicholas in The Princess Diaries 2: Royal Engagement. \n",
      "\n",
      "1. This statement is supported by the context provided.\n",
      "2. No assumptions were made; the information is directly taken from the context.\n",
      "3. The answer directly addresses the question by stating who plays Nicholas.\n",
      "4. No external knowledge was used; the information is based solely on the given context.\n",
      "5. Confidence level: 100%\n",
      "\n",
      "PREDICTION GOLDEN SEMANTIC EVALUATION: The statement is incorrect. The context explicitly states that John Rhys-Davies plays Viscount Mabrey, not Nicholas. Nicholas is a new character in the film, but there is no mention of who plays him in the provided context. \n",
      "\n",
      "**Answer:** 1. This statement is not supported by the context provided.\n",
      "rougeL: 0.27184466019417475, rougeL_golden: 0.06896551724137931\n",
      "bleu: 0.16893910346594, bleu_golden: 0.0\n",
      "bleurt: -0.22481943666934967, bleurt_golden: -0.6595818400382996\n",
      "cosine_similarity: 0.5010708570480347, cosine_similarity_golden: 0.3305445909500122\n",
      "longest match: 0.11204481792717087, longest_match_golden: 0.028011204481792718\n",
      "retrieved_ids: ['doc_2', 'doc_0', 'doc_1', 'doc_28', 'doc_23']\n",
      "relevant_ids: ['doc_0', 'doc_1', 'doc_3', 'doc_18']\n",
      "precision:  {'precision@1': 0.0, 'precision@3': 0.6666666666666666, 'precision@5': 0.4, 'precision@10': 0.2}\n",
      "recall:  {'recall@1': 0.0, 'recall@3': 0.5, 'recall@5': 0.5, 'recall@10': 0.5}\n",
      "map:  {'map@1': 0.0, 'map@3': 0.29166666666666663, 'map@5': 0.29166666666666663, 'map@10': 0.29166666666666663}\n",
      "ndcg:  {'ndcg@1': np.float64(0.0), 'ndcg@3': np.float64(0.5307212739772434), 'ndcg@5': np.float64(0.44149241373678083), 'ndcg@10': np.float64(0.44149241373678083)}\n",
      "\n",
      "\n",
      "===================================== SAMPLE 22 =====================================\n",
      "QUERY: the p-wave phase of an electrocardiogram (ecg) represents\n",
      "\n",
      "GROUND TRUTH: The P wave in the ECG represents atrial depolarization , which results in atrial contraction , or atrial systole .\n",
      "\n",
      "GOLDEN ANSWER: atrial depolarization\n",
      "\n",
      "PREDICTION: The P-wave phase of an electrocardiogram (ECG) represents atrial depolarization, which results in atrial contraction or atrial systole. This is supported by the context, which states, \"The P wave in the ECG represents atrial depolarization, which results in atrial contraction, or atrial systole.\" There are no assumptions made in this answer, as the context explicitly states the role of the P wave. The answer directly addresses the question by stating what the P-wave phase represents, and no external knowledge was used. The confidence level for this answer is high, as the context provides a clear and concise explanation.\n",
      "\n",
      "PREDICTION SEMANTIC EVALUATION: The phrase is correct. The P-wave in the ECG represents atrial depolarization, which leads to atrial contraction or systole, as stated in the context. The term \"phase\" is contextually appropriate here, as both \"wave\" and \"phase\" are used to describe the same ECG component in this context. The answer is accurate and directly supported by the given information. \n",
      "\n",
      "**Confidence Level:** High  \n",
      "**Explanation:** The context explicitly links the P-wave to atrial depolarization and its consequences, and both \"wave\" and \"phase\" are valid terms in this context. The phrase aligns with the provided information without requiring assumptions or external knowledge.\n",
      "PREDICTION GOLDEN: 1. Verification: The context clearly states, \"The P wave in the ECG represents atrial depolarization, which results in atrial contraction, or atrial systole.\"\n",
      "2. Assumptions: None were made beyond what is explicitly stated in the context.\n",
      "3. Answer: The P-wave phase of an electrocardiogram (ECG) represents atrial depolarization, which results in atrial contraction or atrial systole.\n",
      "4. External knowledge: No external knowledge was used.\n",
      "5. Confidence level: High, as the information is directly supported by the context.\n",
      "\n",
      "PREDICTION GOLDEN SEMANTIC EVALUATION: The given phrase is correct. The context explicitly states that the P wave in the ECG represents atrial depolarization, which results in atrial contraction (atrial systole). The answer provided aligns precisely with this information, using equivalent phrasing to convey the same meaning. No external knowledge or assumptions were made, and the confidence level is high due to direct textual support. \n",
      "\n",
      "**Answer:** Yes, the phrase is correct.\n",
      "rougeL: 0.2905982905982906, rougeL_golden: 0.3578947368421052\n",
      "bleu: 0.16270521246012504, bleu_golden: 0.18560112091409675\n",
      "bleurt: 0.10586343705654144, bleurt_golden: -0.09193016588687897\n",
      "cosine_similarity: 0.809409499168396, cosine_similarity_golden: 0.8736274838447571\n",
      "longest match: 0.47368421052631576, longest_match_golden: 0.47368421052631576\n",
      "retrieved_ids: ['doc_0', 'doc_6', 'doc_7', 'doc_1', 'doc_3']\n",
      "relevant_ids: ['doc_0']\n",
      "precision:  {'precision@1': 1.0, 'precision@3': 0.3333333333333333, 'precision@5': 0.2, 'precision@10': 0.1}\n",
      "recall:  {'recall@1': 1.0, 'recall@3': 1.0, 'recall@5': 1.0, 'recall@10': 1.0}\n",
      "map:  {'map@1': 1.0, 'map@3': 1.0, 'map@5': 1.0, 'map@10': 1.0}\n",
      "ndcg:  {'ndcg@1': np.float64(1.0), 'ndcg@3': np.float64(1.0), 'ndcg@5': np.float64(1.0), 'ndcg@10': np.float64(1.0)}\n",
      "\n",
      "\n",
      "===================================== SAMPLE 23 =====================================\n",
      "QUERY: when did star trek the next generation first air\n",
      "\n",
      "GROUND TRUTH: Season Episodes Originally aired First aired Last aired 26 September 28 , 1987 ( 1987 - 09 - 28 ) May 16 , 1988 ( 1988 - 05 - 16 ) 22 November 21 , 1988 ( 1988 - 11 - 21 ) July 17 , 1989 ( 1989 - 07 - 17 ) 26 September 25 , 1989 ( 1989 - 09 - 25 ) June 18 , 1990 ( 1990 - 06 - 18 ) 26 September 24 , 1990 ( 1990 - 09 - 24 ) June 17 , 1991 ( 1991 - 06 - 17 ) 5 26 September 23 , 1991 ( 1991 - 09 - 23 ) June 15 , 1992 ( 1992 - 06 - 15 ) 6 26 September 21 , 1992 ( 1992 - 09 - 21 ) June 21 , 1993 ( 1993 - 06 - 21 ) 7 26 September 20 , 1993 ( 1993 - 09 - 20 ) May 23 , 1994 ( 1994 - 05 - 23 )\n",
      "\n",
      "GOLDEN ANSWER: September 28 , 1987\n",
      "\n",
      "PREDICTION: Star Trek: The Next Generation first aired on September 28, 1987. This information is directly supported by the context provided, which states, \"Star Trek : The Next Generation was announced on October 10, 1986, and its cast in May 1987.\"\n",
      "\n",
      "PREDICTION SEMANTIC EVALUATION: The statement is correct. The first episode of Star Trek: The Next Generation aired on September 28, 1987, as confirmed by the provided context.\n",
      "PREDICTION GOLDEN: **Answer:**  \n",
      "Star Trek: The Next Generation first aired on **September 26, 1987**.  \n",
      "\n",
      "**Support for the answer:**  \n",
      "The context explicitly states \"26 September 28, 1987 (1987 - 09 - 28)\" as the first aired date for Season 1.  \n",
      "\n",
      "**Assumptions checked:**  \n",
      "No assumptions were made beyond the data provided in the table. The answer is directly derived from the given information.  \n",
      "\n",
      "**Confidence level:**  \n",
      "High, as the first aired date is clearly specified in the context.\n",
      "\n",
      "PREDICTION GOLDEN SEMANTIC EVALUATION: **Answer:**  \n",
      "No, the statement is incorrect. The first episode of Star Trek: The Next Generation aired on **September 28, 1987**, not September 26.  \n",
      "\n",
      "**Support for the correction:**  \n",
      "The table explicitly lists \"1987 - 09 - 28\" as the first aired date for Season 1.  \n",
      "\n",
      "**Assumptions checked:**  \n",
      "The answer is directly contradicted by the provided data. No assumptions were made beyond the table's explicit information.  \n",
      "\n",
      "**Confidence level:**  \n",
      "Low, as the date in the table clearly states September 28, 1987.\n",
      "rougeL: 0.0851063829787234, rougeL_golden: 0.10404624277456648\n",
      "bleu: 0.005123670032275687, bleu_golden: 0.06222651170385406\n",
      "bleurt: -0.597043514251709, bleurt_golden: -0.6187096834182739\n",
      "cosine_similarity: 0.2948610782623291, cosine_similarity_golden: 0.4801138639450073\n",
      "longest match: 0.022033898305084745, longest_match_golden: 0.025423728813559324\n",
      "retrieved_ids: ['doc_36', 'doc_156', 'doc_155', 'doc_10', 'doc_154']\n",
      "relevant_ids: ['doc_3', 'doc_8', 'doc_14']\n",
      "precision:  {'precision@1': 0.0, 'precision@3': 0.0, 'precision@5': 0.0, 'precision@10': 0.0}\n",
      "recall:  {'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0}\n",
      "map:  {'map@1': 0.0, 'map@3': 0.0, 'map@5': 0.0, 'map@10': 0.0}\n",
      "ndcg:  {'ndcg@1': np.float64(0.0), 'ndcg@3': np.float64(0.0), 'ndcg@5': np.float64(0.0), 'ndcg@10': np.float64(0.0)}\n",
      "\n",
      "\n",
      "===================================== SAMPLE 32 =====================================\n",
      "QUERY: who sang the most wonderful summer of my life\n",
      "\n",
      "GROUND TRUTH: Jackie Ward ( born Jacqueline McDonnell , 1941 ) , better known as Robin Ward , is an American singer , regarded as a `` one - hit wonder '' of 1963 million - selling song `` Wonderful Summer '' . However , using her real name she was highly accomplished and successful singing in groups . Ward 's voice is heard in U.S. television series , motion pictures , advertisements , and pop records . She is one of the real singers of the hits attributed to The Partridge Family .\n",
      "\n",
      "GOLDEN ANSWER: Jackie Ward\n",
      "\n",
      "PREDICTION: Based on the provided context, it is not possible to determine who sang \"The Most Wonderful Summer of My Life\" because this song is not mentioned in the context. The context only mentions \"Wonderful Summer\" by Jackie Ward, who is referred to as Robin Ward. The context does not provide any information about other songs or artists associated with \"The Most Wonderful Summer of My Life.\" Therefore, the answer is not supported by the context. The confidence level is 0%.\n",
      "\n",
      "PREDICTION SEMANTIC EVALUATION: The statement is correct. The context does not mention \"The Most Wonderful Summer of My Life,\" and there is no explicit connection to Jackie Ward (Robin Ward) or The Partridge Family in the provided information. The context only discusses \"Wonderful Summer,\" making it impossible to determine who sang \"The Most Wonderful Summer of My Life\" from the given data. The confidence level is **0%**.\n",
      "PREDICTION GOLDEN: The context states that Jackie Ward, better known as Robin Ward, sang \"Wonderful Summer,\" which was a million-selling song in 1963. However, the question asks about \"The Most Wonderful Summer of My Life,\" which is not mentioned in the context. Based on the information provided, we cannot confirm that Jackie Ward sang \"The Most Wonderful Summer of My Life.\" The context does not provide any information about this song or her involvement with it.\n",
      "\n",
      "1. The statement about Jackie Ward singing \"Wonderful Summer\" is supported by the context.\n",
      "2. No assumptions were made, as the context does not mention any other songs or performances.\n",
      "3. The answer does not directly address the question because \"The Most Wonderful Summer of My Life\" is not mentioned in the context.\n",
      "4. No external knowledge was used, as the answer was derived directly from the provided context.\n",
      "5. Confidence level assessment: Low (The answer cannot be confirmed due to lack of information in the context).\n",
      "\n",
      "PREDICTION GOLDEN SEMANTIC EVALUATION: The context does not mention \"The Most Wonderful Summer of My Life,\" so we cannot confirm whether Jackie Ward sang it. The confidence level is low because the statement is not supported by the provided information. \n",
      "\n",
      "**Answer:**  \n",
      "3. The answer does not directly address the question because \"The Most Wonderful Summer of My Life\" is not mentioned in the context.\n",
      "rougeL: 0.14379084967320263, rougeL_golden: 0.17872340425531916\n",
      "bleu: 0.0, bleu_golden: 0.05270233265649254\n",
      "bleurt: -0.7859230637550354, bleurt_golden: -0.4697810113430023\n",
      "cosine_similarity: 0.6203927993774414, cosine_similarity_golden: 0.6916660666465759\n",
      "longest match: 0.03805496828752643, longest_match_golden: 0.05919661733615222\n",
      "retrieved_ids: ['doc_7', 'doc_5', 'doc_1', 'doc_8', 'doc_2']\n",
      "relevant_ids: ['doc_0', 'doc_1', 'doc_5', 'doc_7', 'doc_8', 'doc_9', 'doc_17', 'doc_18']\n",
      "precision:  {'precision@1': 1.0, 'precision@3': 1.0, 'precision@5': 0.8, 'precision@10': 0.4}\n",
      "recall:  {'recall@1': 0.125, 'recall@3': 0.375, 'recall@5': 0.5, 'recall@10': 0.5}\n",
      "map:  {'map@1': 0.125, 'map@3': 0.375, 'map@5': 0.5, 'map@10': 0.5}\n",
      "ndcg:  {'ndcg@1': np.float64(1.0), 'ndcg@3': np.float64(1.0), 'ndcg@5': np.float64(0.8687949224876582), 'ndcg@10': np.float64(0.647939623894138)}\n",
      "\n",
      "\n",
      "===================================== SAMPLE 37 =====================================\n",
      "QUERY: how many oscars did on golden pond win\n",
      "\n",
      "GROUND TRUTH: The film received ten nominations at the 54th Academy Awards including Best Picture and won three : Best Actor ( Fonda ) , Best Actress ( Hepburn ) and Best Adapted Screenplay ( Thompson ) . Henry Fonda won his only competitive Oscar with this movie and at the age of 76 , he became the oldest winner in the aforementioned category . Katharine Hepburn won her fourth Best Actress award , extending her own record for the most Oscars won by a thespian . On Golden Pond is also one of the few movies to earn the nominations for five major Academy Awards ( Best Picture , Director , Actor , Actress , and Screenplay ) .\n",
      "\n",
      "GOLDEN ANSWER: three\n",
      "\n",
      "PREDICTION: On Golden Pond won 5 Academy Awards. The context states that the film earned nominations for five major Academy Awards (Best Picture, Director, Actor, Actress, and Screenplay). However, it does not explicitly mention how many Oscars it won. Based on the information provided, we cannot directly confirm the number of Oscars On Golden Pond won. The context only provides the number of nominations.\n",
      "\n",
      "PREDICTION SEMANTIC EVALUATION: The statement is not confirmed as correct. The context only mentions that On Golden Pond earned **nominations** for five major Academy Awards but does not specify how many **awards** it won. Without explicit information on the number of wins, the claim that it \"won 5 Academy Awards\" cannot be directly verified.\n",
      "PREDICTION GOLDEN: On Golden Pond won three Oscars, specifically for Best Actor (Henry Fonda), Best Actress (Katharine Hepburn), and Best Adapted Screenplay (Thompson). This answer is directly supported by the context provided, as it states that the film received ten nominations and won three awards, which are specifically named. No assumptions were made, and the answer addresses the question directly by stating the number of Oscars the film won.\n",
      "\n",
      "PREDICTION GOLDEN SEMANTIC EVALUATION: The statement is correct. It accurately lists the three Oscars won by \"On Golden Pond\": Best Actor (Henry Fonda), Best Actress (Katharine Hepburn), and Best Adapted Screenplay (Thompson), as specified in the provided context.\n",
      "rougeL: 0.2608695652173913, rougeL_golden: 0.2545454545454545\n",
      "bleu: 0.18266580947536362, bleu_golden: 0.16392659676696894\n",
      "bleurt: -0.5573936700820923, bleurt_golden: -0.3068274259567261\n",
      "cosine_similarity: 0.7352968454360962, cosine_similarity_golden: 0.8730745911598206\n",
      "longest match: 0.07142857142857142, longest_match_golden: 0.05519480519480519\n",
      "retrieved_ids: ['doc_5', 'doc_44', 'doc_45', 'doc_26', 'doc_1']\n",
      "relevant_ids: ['doc_4', 'doc_31', 'doc_50']\n",
      "precision:  {'precision@1': 0.0, 'precision@3': 0.0, 'precision@5': 0.0, 'precision@10': 0.0}\n",
      "recall:  {'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0}\n",
      "map:  {'map@1': 0.0, 'map@3': 0.0, 'map@5': 0.0, 'map@10': 0.0}\n",
      "ndcg:  {'ndcg@1': np.float64(0.0), 'ndcg@3': np.float64(0.0), 'ndcg@5': np.float64(0.0), 'ndcg@10': np.float64(0.0)}\n",
      "\n",
      "\n",
      "===================================== SAMPLE 39 =====================================\n",
      "QUERY: who won season 4 of america's got talent\n",
      "\n",
      "GROUND TRUTH: The fourth season of America 's Got Talent , an American television reality show talent competition , premiered on the NBC network on June 23 , 2009 . Country singer Kevin Skinner was named the winner on September 16 , 2009 . This season is the first season to be broadcast in high definition .\n",
      "\n",
      "GOLDEN ANSWER: Kevin Skinner\n",
      "\n",
      "PREDICTION: Based on the context provided, Kevin Skinner won Season 4 of America's Got Talent. The context explicitly states, \"This season is the first season to be broadcast in high definition. This season 's host is Nick Cannon, who replaced Jerry Springer. Springer had said that he could not host due to other commitments. David Hasselhoff, Piers Morgan and Sharon Osbourne returned judges, despite rumors. Country singer Kevin Skinner was named the winner on September 16, 2009.\"\n",
      "\n",
      "PREDICTION SEMANTIC EVALUATION: Yes, the statement is correct. The context explicitly states that Kevin Skinner was named the winner of Season 4 of America's Got Talent on September 16, 2009.\n",
      "PREDICTION GOLDEN: 1. Verification: The context states, \"The fourth season of America's Got Talent ... premiered on the NBC network on June 23, 2009. Country singer Kevin Skinner was named the winner on September 16, 2009.\"\n",
      "2. Assumptions: No assumptions were made beyond the information provided in the context.\n",
      "3. Direct answer: Kevin Skinner won season 4 of America's Got Talent.\n",
      "4. External knowledge: No external knowledge was used.\n",
      "5. Confidence level: High, as the information is explicitly stated in the context.\n",
      "\n",
      "PREDICTION GOLDEN SEMANTIC EVALUATION: Yes, the statement is correct. The context explicitly states, \"Country singer Kevin Skinner was named the winner on September 16, 2009.\" This directly confirms that Kevin Skinner won the fourth season of America's Got Talent. The other points in your provided information (e.g., confidence level, assumptions, external knowledge) align with the given data, reinforcing the accuracy of the answer.\n",
      "rougeL: 0.32, rougeL_golden: 0.49612403100775193\n",
      "bleu: 0.3173567305940898, bleu_golden: 0.2728690888854965\n",
      "bleurt: -0.14299432933330536, bleurt_golden: -0.23499418795108795\n",
      "cosine_similarity: 0.7450547814369202, cosine_similarity_golden: 0.7889700531959534\n",
      "longest match: 0.22789115646258504, longest_match_golden: 0.22789115646258504\n",
      "retrieved_ids: ['doc_105', 'doc_0', 'doc_93', 'doc_59', 'doc_1']\n",
      "relevant_ids: ['doc_0', 'doc_1', 'doc_44', 'doc_55', 'doc_61', 'doc_62', 'doc_63', 'doc_105']\n",
      "precision:  {'precision@1': 1.0, 'precision@3': 0.6666666666666666, 'precision@5': 0.6, 'precision@10': 0.3}\n",
      "recall:  {'recall@1': 0.125, 'recall@3': 0.25, 'recall@5': 0.375, 'recall@10': 0.375}\n",
      "map:  {'map@1': 0.125, 'map@3': 0.25, 'map@5': 0.325, 'map@10': 0.325}\n",
      "ndcg:  {'ndcg@1': np.float64(1.0), 'ndcg@3': np.float64(0.7653606369886217), 'ndcg@5': np.float64(0.6843515475204855), 'ndcg@10': np.float64(0.510383374527714)}\n"
     ]
    }
   ],
   "source": [
    "for r in results:\n",
    "    print(f\"\\n\\n===================================== SAMPLE {r['index']} =====================================\")\n",
    "    print(f\"QUERY: {r['query']}\\n\")\n",
    "    print(f\"GROUND TRUTH: {r['golden_context']}\\n\")\n",
    "    print(f\"GOLDEN ANSWER: {r['gold_answer']}\\n\")\n",
    "    if not only_retrieve:\n",
    "        print(f\"PREDICTION: {r['prediction']}\\n\")\n",
    "        print(f\"PREDICTION SEMANTIC EVALUATION: {r['semantic_evaluation']}\")\n",
    "        print(f\"PREDICTION GOLDEN: {r['prediction_golden']}\\n\")\n",
    "        print(f\"PREDICTION GOLDEN SEMANTIC EVALUATION: {r['semantic_evaluation_golden']}\")\n",
    "        print(f\"rougeL: {r['rougeL']}, rougeL_golden: {r['rougeL_golden']}\")\n",
    "        print(f\"bleu: {r['bleu']}, bleu_golden: {r['bleu_golden']}\")\n",
    "        print(f\"bleurt: {r['bleurt']}, bleurt_golden: {r['bleurt_golden']}\")\n",
    "        print(f\"cosine_similarity: {r['cosine_similarity']}, cosine_similarity_golden: {r['cosine_similarity_golden']}\")\n",
    "        print(f\"longest match: {r['longest_match']}, longest_match_golden: {r['longest_match_golden']}\")\n",
    "    print(\"retrieved_ids:\", r['retrieved_ids'])\n",
    "    print(\"relevant_ids:\", r['relevant_ids'])\n",
    "    print(\"precision: \", r['precision_dict'])\n",
    "    print(\"recall: \", r['recall_dict'])\n",
    "    print(\"map: \", r['map_dict'])\n",
    "    print(\"ndcg: \", r['ndcg_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1457,
   "id": "258f9d60c1895cf1",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "output_folder = config[\"output_folder\"]\n",
    "name = config[\"run_name\"]\n",
    "run_name = f\"{name}_{datetime.now().strftime('%Y_%m_%d_%H%M%S')}\"\n",
    "out_path = os.path.join(output_folder, run_name)\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Split metrics into LLM (generation) and retrieval\n",
    "general_infos = [\n",
    "    \"query\", \"gold_answer\", \"golden_context\", \"elapsed_time\"\n",
    "]\n",
    "llm_data = [\n",
    "    \"prediction\", \"prediction_golden\", \"semantic_evaluation\", \"semantic_evaluation_golden\"\n",
    "]\n",
    "llm_metrics = [\n",
    "    \"rougeL\", \"rougeL_golden\", \"bleu\", \"bleu_golden\", \"bleurt\", \"bleurt_golden\",\n",
    "    \"cosine_similarity\", \"cosine_similarity_golden\", \"longest_match\", \"longest_match_golden\"\n",
    "]\n",
    "retrieval_data = [\n",
    "    \"relevant_ids\", \"retrieved_ids\"\n",
    "]\n",
    "retrieval_metrics = [\"precision_dict\", \"recall_dict\", \"map_dict\", \"ndcg_dict\"]\n",
    "\n",
    "# Always include these columns\n",
    "common_cols = [\"index\"]\n",
    "\n",
    "# Save general infos of test run\n",
    "general_infos_df = results_df[common_cols + general_infos]\n",
    "general_infos_json = general_infos_df.to_dict(orient=\"records\")\n",
    "with open(os.path.join(out_path, \"queries.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(general_infos_json, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Save LLM metrics\n",
    "if not only_retrieve:\n",
    "    results_df[llm_metrics].to_csv(os.path.join(out_path, \"generation_metrics.csv\"), index=False)\n",
    "    # Save LLM responses\n",
    "    llm_data_df = results_df[common_cols + llm_data]\n",
    "    general_infos_json = llm_data_df.to_dict(orient=\"records\")\n",
    "    with open(os.path.join(out_path, \"responses.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(general_infos_json, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Save retrieval metrics\n",
    "sums = {m: defaultdict(float) for m in retrieval_metrics}\n",
    "counts = {m: defaultdict(int) for m in retrieval_metrics}\n",
    "\n",
    "for r in results:\n",
    "    for m in retrieval_metrics:\n",
    "        for k, v in r[m].items():\n",
    "            sums[m][k] += float(v)\n",
    "            counts[m][k] += 1\n",
    "\n",
    "means = {\n",
    "    m: {k: (sums[m][k] / counts[m][k]) for k in sums[m]}\n",
    "    for m in retrieval_metrics\n",
    "}\n",
    "\n",
    "means_df = pd.DataFrame()\n",
    "means_df[\"k\"] = config[\"k_values\"]\n",
    "for _, m in means.items():\n",
    "    metric = next(iter(m.keys()))\n",
    "    metric = metric.split(\"@\")[0]\n",
    "    values = m.values()\n",
    "    means_df[metric] = values\n",
    "means_df.to_csv(os.path.join(out_path, \"retrieval_metrics.csv\"), index=False)\n",
    "# Save retrieved docs\n",
    "llm_data_df = results_df[common_cols + retrieval_data]\n",
    "general_infos_json = llm_data_df.to_dict(orient=\"records\")\n",
    "with open(os.path.join(out_path, \"retrieved_docs.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(general_infos_json, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Save config\n",
    "with open(os.path.join(out_path, \"used_config.json\"), \"w\") as f:\n",
    "    json.dump(config, f, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
