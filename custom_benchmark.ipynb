{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57f51bb443694561",
   "metadata": {},
   "source": [
    "# RAG Benchmark Evaluation on Natural Questions\n",
    "\n",
    "This notebook evaluates a RAG (Retrieval-Augmented Generation) system on the Natural Questions dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea118ea9e434089f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Setup and Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60407f4d1af3bd96",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Imports and dependencies\n",
    "\n",
    "Import all necessary libraries for dataset loading, embeddings, LLM, text processing, and evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4eea478c45fbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.document import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841ce3ea6db11221",
   "metadata": {},
   "source": [
    "### Config File\n",
    "\n",
    "Configure the dataset split, output folder, embedding model, chunk size, overlap, and LLM parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf0ebd2fe01be71",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"configs/custom_benchmark_config.json\"\n",
    "\n",
    "with open(config_file, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "only_retrieve = config[\"only_retrieve\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872e3764aa05cf93",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load API Key\n",
    "\n",
    "Load the Hugging Face API key from environment variables to authenticate model requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e3df368ffe7436",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T16:33:00.567283Z",
     "start_time": "2025-07-02T16:33:00.561242Z"
    }
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "HF_API_KEY = os.getenv(\"API_KEY10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d918d513f3e953",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load dataset\n",
    "\n",
    "Load the Natural Questions dataset subset for benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292d18be6927d28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T16:33:05.613036Z",
     "start_time": "2025-07-02T16:33:00.568727Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"natural_questions\", split=config[\"dataset_split\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8a0c45ab8fd77c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Model Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ec0c6bbede92a4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Embeddings initialization\n",
    "\n",
    "Create the embedding model for converting text chunks into vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733553b79138a2ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T16:33:07.494706Z",
     "start_time": "2025-07-02T16:33:05.614373Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=config[\"embedding_model_name\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8989ddb91f0dfa0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Text splitter configuration\n",
    "\n",
    "Define how the document is split into overlapping chunks for embedding and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ec7696c36ded36",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=config[\"chunk_size\"],\n",
    "    chunk_overlap=config[\"chunk_overlap\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75658f377b18de7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### LLM setup\n",
    "\n",
    "Configure the language model endpoint on Hugging Face Hub for text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604630da582b0af3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T16:33:07.505915Z",
     "start_time": "2025-07-02T16:33:07.501012Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "if not only_retrieve:\n",
    "    llm = HuggingFaceEndpoint(\n",
    "        repo_id=config[\"llm_model\"],\n",
    "        huggingfacehub_api_token=HF_API_KEY,\n",
    "        task=\"text-generation\",\n",
    "        temperature=config[\"temperature\"],\n",
    "        max_new_tokens=config[\"max_new_tokens\"],\n",
    "    )\n",
    "    chat_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9983a91bdc30f78",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Retrieval and querying\n",
    "\n",
    "Functions to retrieve relevant documents locally and query the LLM with that context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b6e3f8315f79b6",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def retrieve_local(query, vectorstore, k=config[\"top_k\"]):\n",
    "    docs_faiss = vectorstore.similarity_search(query, k=k)\n",
    "    return [d.page_content for d in docs_faiss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f3fb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_doc_ids(split_docs, golden_answer=None):\n",
    "    relevant_ids = []\n",
    "    for doc in split_docs:\n",
    "        content = doc.page_content.lower()\n",
    "        if golden_answer and golden_answer.strip() and golden_answer.strip().lower() in content:\n",
    "            relevant_ids.append(doc.metadata[\"doc_id\"])\n",
    "    return relevant_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d2661109411a70",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def ask(query, context):\n",
    "    messages = [\n",
    "        SystemMessage(content=f\"Just answer queries based on {context}.\"),\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        Answer the query: {query} based uniquely on the context: {context}, don't make up anything, just say what the context contains. If the information is not in the context, you must say you don't know. You must answer only the specified question and nothing else.\n",
    "        \"\"\")\n",
    "    ]\n",
    "    response = chat_model.invoke(messages)\n",
    "    cleaned = re.sub(r\"<think>[\\s\\S]*?</think>\\s*\", \"\", response.content, flags=re.DOTALL)\n",
    "    return cleaned.strip()\n",
    "\n",
    "#ask(\"What is the capital of France?\", \"France is a country in Europe.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c32260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_think_tags(text):\n",
    "    cleaned = re.sub(r\"\\s*<think>[\\s\\S]*?</think>\\s*\", \"\", text, flags=re.DOTALL)\n",
    "    return cleaned.strip()\n",
    "\n",
    "sample = \"\"\"\n",
    "<think>\n",
    "Okay, let me try to figure this out. The user is asking whether the statement \"The context contains information about the 'Now That's What I Call Music ( original UK album )', which is referenced in the statement \"Now That's What I Call Music (\", indicating that it is related to the album series. However, it does not specify the exact release year of the original UK album. To provide a precise answer, I would need to refer to external sources or more detailed context. Based on the information provided, I do not know the exact release year of the original UK album.\"\n",
    "\n",
    "First, I need to parse the given text. The text mentions that the album was released in the UK on 28 November 1983. The user is saying that the context provided doesn't specify the exact release year of the original UK album. Wait, but the text does say 28 November 1983. So the user is pointing out that even though the text mentions the release date, they are saying that the context doesn't specify it. That seems contradictory.\n",
    "\n",
    "Wait, the user's statement is a bit confusing. Let me read it again. The user is saying that the context (the text provided) contains information about the \"Now That's What I Call Music ( original UK album )\", which is referenced in the statement \"Now That's What I Call Music (\", indicating that it's related to the series. However, the context does not specify the exact release year. But the text clearly states the release date as 28 November 1983. So the user is saying that the context doesn't specify it, but the text does. That seems like a contradiction. \n",
    "\n",
    "Wait, maybe the user is saying that the context (the text) does not specify the release year, but the user is aware that the text does. So the user is pointing out that the context (the text) doesn't have the release year, but in reality, the text does. Therefore, the user's statement is incorrect because the text does specify the release year. \n",
    "\n",
    "Alternatively, maybe the user is confused. Let me check the original text again. The original text says: \"Initial pressings were released on vinyl and audio cassette. To celebrate the 25th anniversary of the album and series, the album was re-released on CD for the first time in 2009. However, alternative longer mixes of Only For Love, Double Dutch and Candy Girl were included in place of the original shorter single mixes from 1983. A double vinyl re-release followed for Record Store Day on 18 April 2015. In July 2018, the album was newly remastered and re-released on CD, vinyl and cassette to commemorate the release of the 100th volume of the series.\"\n",
    "\n",
    "So the original release date is mentioned as 28 November 1983. Therefore, the user's statement that the context does not specify the exact release year is incorrect because the text does specify it. Therefore, the answer is that the user's statement is incorrect because the context does provide the release year. The user is wrong in their assertion that the context does not specify it. The correct answer is that the user's statement is incorrect because the text does include the release year.\n",
    "</think>babaab\n",
    "balblablalbalbalb\n",
    "\"\"\"\n",
    "\n",
    "print(clean_think_tags(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2ebed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_check(query, ground_truth, prediction):\n",
    "    messages = [\n",
    "        #SystemMessage(content=f\"Just answer queries based on {ground_truth}.\"),\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        {ground_truth}\n",
    "\n",
    "        given the previous statements, you MUST say (shortly) if the following phrase is correct\n",
    "\n",
    "        {prediction}\n",
    "        \"\"\")\n",
    "    ]\n",
    "    response = chat_model.invoke(messages)\n",
    "    cleaned = re.sub(r\"\\s*<think>[\\s\\S]*?</think>\\s*\", \"\", response.content, flags=re.DOTALL)\n",
    "    return cleaned.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9216a27f27bb44",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data extraction and preprocessing\n",
    "\n",
    "Functions to extract valid answers from dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd55c4e0a2d91e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T16:33:07.525093Z",
     "start_time": "2025-07-02T16:33:07.517256Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def extract_answers(sample):\n",
    "    tokens = sample[\"document\"][\"tokens\"]\n",
    "    short_answer = \"\"\n",
    "    start = sample[\"annotations\"][\"short_answers\"][0][\"start_token\"]\n",
    "    end = sample[\"annotations\"][\"short_answers\"][0][\"end_token\"]\n",
    "    if len(start) > 0:\n",
    "        short_answer = \" \".join([\n",
    "            t for t, html in zip(tokens[\"token\"][int(start[0]):int(end[0])], tokens[\"is_html\"][start[0]:end[0]]) if not html\n",
    "        ])\n",
    "\n",
    "    long_answer = \"\"\n",
    "    if sample[\"annotations\"][\"long_answer\"][0][\"start_token\"] != -1:\n",
    "        start = sample[\"annotations\"][\"long_answer\"][0][\"start_token\"]\n",
    "        end = sample[\"annotations\"][\"long_answer\"][0][\"end_token\"]\n",
    "        long_answer = \" \".join([\n",
    "            t for t, html in zip(tokens[\"token\"][start:end], tokens[\"is_html\"][start:end]) if not html\n",
    "        ])\n",
    "\n",
    "    return long_answer or \"\", short_answer or \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fa481a60693a36",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def preprocess_text(sample):\n",
    "    tokens = sample[\"document\"][\"tokens\"]\n",
    "    return \" \".join([t for t, html in zip(tokens[\"token\"], tokens[\"is_html\"]) if not html])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b739fec4387fcc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Benchmarking "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5901ab8d628195c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Retrieve benchmark\n",
    "\n",
    "By finding the longest match between the prediction and the golden context, we can evaluate how well the model retrieves relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2c257fcdb1d26b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T16:33:07.538105Z",
     "start_time": "2025-07-02T16:33:07.532475Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def find_longest_match(str1, str2):\n",
    "    max_len = 0\n",
    "    len2 = len(str2)\n",
    "\n",
    "    for i in range(len2):\n",
    "        for j in range(i + 1, len2 + 1):\n",
    "            substr = str2[i:j]\n",
    "            if substr in str1 and len(substr) > max_len:\n",
    "                max_len = len(substr)\n",
    "\n",
    "    return max_len / len2 if len2 > 0 else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5dc984b0153086",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Evaluation benchmark\n",
    "\n",
    "Load rouge and bleu metrics for evaluating the model's predictions against the golden answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76c5209da15957e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T16:33:10.160892Z",
     "start_time": "2025-07-02T16:33:07.540605Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "if not only_retrieve:\n",
    "    metric_rouge = evaluate.load(\"rouge\")\n",
    "    metric_bleu = evaluate.load(\"bleu\")\n",
    "    metric_bleurt = evaluate.load(\"bleurt\", \"bleurt-large-512\")\n",
    "    comparison_model = SentenceTransformer(config[\"embedding_model_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23f2a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(retrieved, relevant, k):\n",
    "    retrieved_k = retrieved[:k]\n",
    "    return len(set(retrieved_k) & set(relevant)) / k\n",
    "\n",
    "def recall_at_k(retrieved, relevant, k):\n",
    "    retrieved_k = retrieved[:k]\n",
    "    return len(set(retrieved_k) & set(relevant)) / len(relevant) if relevant else 0\n",
    "\n",
    "def average_precision(retrieved, relevant, k):\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "    for i, doc_id in enumerate(retrieved[:k]):\n",
    "        if doc_id in relevant:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "    return score / len(relevant) if relevant else 0\n",
    "\n",
    "def dcg_at_k(retrieved, relevant, k):\n",
    "    dcg = 0.0\n",
    "    for i, doc_id in enumerate(retrieved[:k]):\n",
    "        if doc_id in relevant:\n",
    "            dcg += 1.0 / np.log2(i + 2)\n",
    "    return dcg\n",
    "\n",
    "def ndcg_at_k(retrieved, relevant, k):\n",
    "    dcg = dcg_at_k(retrieved, relevant, k)\n",
    "    idcg = sum(1.0 / np.log2(i + 2) for i in range(min(len(relevant), k)))\n",
    "    return dcg / idcg if idcg > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72a8cb8659c09c9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Single sample processing\n",
    "\n",
    "Process one sample: build context, eventually query LLM, extract answer, and compute metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dd8c46c6a1cb9e",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def process_sample(i, sample):\n",
    "    k_values = config.get(\"k_values\", [1, 3, 5, 10])\n",
    "    golden_context, golden_answer = extract_answers(sample)\n",
    "    if golden_answer == \"\" or golden_context == \"\":\n",
    "        return None\n",
    "    \n",
    "    query = sample[\"question\"][\"text\"]\n",
    "\n",
    "    text = preprocess_text(sample)\n",
    "    docs = [Document(page_content=text)]\n",
    "    split_docs = splitter.split_documents(docs)\n",
    "    for idx, doc in enumerate(split_docs):\n",
    "        doc.metadata = {\"doc_id\": f\"doc_{idx}\"}\n",
    "    vectorstore = FAISS.from_documents(split_docs, embedding_model)\n",
    "\n",
    "    docs_faiss = vectorstore.similarity_search(query, k=config['top_k'])\n",
    "    retrieved_ids = [d.metadata[\"doc_id\"] for d in docs_faiss]\n",
    "\n",
    "    relevant_ids = get_relevant_doc_ids(split_docs, golden_answer)\n",
    "\n",
    "    precision_dict = {}\n",
    "    recall_dict = {}\n",
    "    map_dict = {}\n",
    "    ndcg_dict = {}\n",
    "    for k in k_values:\n",
    "        precision_dict[f\"precision@{k}\"] = precision_at_k(retrieved_ids, relevant_ids, k)\n",
    "        recall_dict[f\"recall@{k}\"] = recall_at_k(retrieved_ids, relevant_ids, k)\n",
    "        map_dict[f\"map@{k}\"] = average_precision(retrieved_ids, relevant_ids, k)\n",
    "        ndcg_dict[f\"ndcg@{k}\"] = ndcg_at_k(retrieved_ids, relevant_ids, k)\n",
    "\n",
    "    if not only_retrieve:\n",
    "        golden_embeddings = comparison_model.encode(golden_context, convert_to_tensor=True)\n",
    "        \n",
    "        context = retrieve_local(query, vectorstore)\n",
    "        prediction = ask(query, context)\n",
    "        \n",
    "        rouge = metric_rouge.compute(predictions=[prediction], references=[golden_context])\n",
    "        bleu = metric_bleu.compute(predictions=[prediction], references=[golden_context])\n",
    "        bleurt = metric_bleurt.compute(predictions=[prediction], references=[golden_context])\n",
    "        longest_match = find_longest_match(prediction, golden_context)\n",
    "        pred_embeddings = comparison_model.encode(prediction, convert_to_tensor=True)\n",
    "        cosine_similarity = util.pytorch_cos_sim(pred_embeddings, golden_embeddings)\n",
    "        semantic_evaluation = semantic_check(query, golden_context, prediction)\n",
    "\n",
    "        prediction_golden = ask(query, golden_context)\n",
    "            \n",
    "        rouge_golden = metric_rouge.compute(predictions=[prediction_golden], references=[golden_context])\n",
    "        bleu_golden = metric_bleu.compute(predictions=[prediction_golden], references=[golden_context])\n",
    "        bleurt_golden = metric_bleurt.compute(predictions=[prediction_golden], references=[golden_context])\n",
    "        longest_match_golden = find_longest_match(prediction_golden, golden_context)\n",
    "        pred_embeddings_golden = comparison_model.encode(prediction_golden, convert_to_tensor=True)\n",
    "        cosine_similarity_golden = util.pytorch_cos_sim(pred_embeddings_golden, golden_embeddings)\n",
    "        semantic_evaluation_golden = semantic_check(query, golden_context, prediction_golden)\n",
    "    \n",
    "        result = {\n",
    "            \"index\": i,\n",
    "            \"rougeL\": rouge[\"rougeL\"],\n",
    "            \"rougeL_golden\": rouge_golden[\"rougeL\"],\n",
    "            \"bleu\": bleu[\"bleu\"],\n",
    "            \"bleu_golden\": bleu_golden[\"bleu\"],\n",
    "            \"bleurt\": bleurt[\"scores\"][0],\n",
    "            \"bleurt_golden\": bleurt_golden[\"scores\"][0],\n",
    "            \"cosine_similarity\": cosine_similarity.item(),\n",
    "            \"cosine_similarity_golden\": cosine_similarity_golden.item(),\n",
    "            \"longest_match\": longest_match,\n",
    "            \"longest_match_golden\": longest_match_golden,\n",
    "            \"query\": query,\n",
    "            \"gold_answer\": golden_answer,\n",
    "            \"golden_context\": golden_context,\n",
    "            \"prediction\": prediction,\n",
    "            \"prediction_golden\": prediction_golden,\n",
    "            \"semantic_evaluation\": semantic_evaluation,\n",
    "            \"semantic_evaluation_golden\": semantic_evaluation_golden,\n",
    "            \"retrieved_docs\": context,\n",
    "            \"relevant_ids\": relevant_ids,\n",
    "            \"retrieved_ids\": retrieved_ids,\n",
    "            \"precision_dict\": precision_dict,\n",
    "            \"recall_dict\": recall_dict,\n",
    "            \"map_dict\": map_dict,\n",
    "            \"ndcg_dict\": ndcg_dict\n",
    "        }\n",
    "    else:\n",
    "        result = {\n",
    "            \"index\": i,\n",
    "            \"query\": query,\n",
    "            \"gold_answer\": golden_answer,\n",
    "            \"golden_context\": golden_context,\n",
    "            \"relevant_ids\": relevant_ids,\n",
    "            \"retrieved_ids\": retrieved_ids,\n",
    "            \"precision_dict\": precision_dict,\n",
    "            \"recall_dict\": recall_dict,\n",
    "            \"map_dict\": map_dict,\n",
    "            \"ndcg_dict\": ndcg_dict\n",
    "        }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b8db3a1ad908b0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Full benchmark loop\n",
    "\n",
    "Run the evaluation until the target number of valid samples with answers is reached, skipping samples with empty references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c273ca2913c053a2",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "num_valid_examples = config[\"num_valid_examples\"]\n",
    "\n",
    "results = []\n",
    "i = 0\n",
    "\n",
    "with tqdm(total=num_valid_examples) as pbar:\n",
    "    while len(results) < num_valid_examples and i < len(dataset):\n",
    "        sample = dataset[i]\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        result = process_sample(i, sample)\n",
    "        elapsed_time = time.perf_counter() - start_time\n",
    "            \n",
    "        if result:\n",
    "            result[\"elapsed_time\"] = elapsed_time\n",
    "            results.append(result)\n",
    "            pbar.update(1)\n",
    "            \n",
    "        i += 1\n",
    "\n",
    "print(f\"Processed {len(results)} valid samples out of {i} total samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2581af0cde6a2c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Save and display results\n",
    "\n",
    "Print average metric scores, save data to CSV and JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ac12b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in results:\n",
    "    print(f\"\\n\\n===================================== SAMPLE {r['index']} =====================================\")\n",
    "    print(f\"QUERY: {r['query']}\\n\")\n",
    "    print(f\"GROUND TRUTH: {r['golden_context']}\\n\")\n",
    "    print(f\"GOLDEN ANSWER: {r['gold_answer']}\\n\")\n",
    "    if not only_retrieve:\n",
    "        print(f\"PREDICTION: {r['prediction']}\\n\")\n",
    "        print(f\"PREDICTION SEMANTIC EVALUATION: {r['semantic_evaluation']}\")\n",
    "        print(f\"PREDICTION GOLDEN: {r['prediction_golden']}\\n\")\n",
    "        print(f\"PREDICTION GOLDEN SEMANTIC EVALUATION: {r['semantic_evaluation_golden']}\")\n",
    "        print(f\"rougeL: {r['rougeL']}, rougeL_golden: {r['rougeL_golden']}\")\n",
    "        print(f\"bleu: {r['bleu']}, bleu_golden: {r['bleu_golden']}\")\n",
    "        print(f\"bleurt: {r['bleurt']}, bleurt_golden: {r['bleurt_golden']}\")\n",
    "        print(f\"cosine_similarity: {r['cosine_similarity']}, cosine_similarity_golden: {r['cosine_similarity_golden']}\")\n",
    "        print(f\"longest match: {r['longest_match']}, longest_match_golden: {r['longest_match_golden']}\")\n",
    "    print(\"retrieved_ids:\", r['retrieved_ids'])\n",
    "    print(\"relevant_ids:\", r['relevant_ids'])\n",
    "    print(\"precision: \", r['precision_dict'])\n",
    "    print(\"recall: \", r['recall_dict'])\n",
    "    print(\"map: \", r['map_dict'])\n",
    "    print(\"ndcg: \", r['ndcg_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258f9d60c1895cf1",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "output_folder = config[\"output_folder\"]\n",
    "name = config[\"run_name\"]\n",
    "run_name = f\"{name}_{datetime.now().strftime('%Y_%m_%d_%H%M%S')}\"\n",
    "out_path = os.path.join(output_folder, run_name)\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Split metrics into LLM (generation) and retrieval\n",
    "general_infos = [\n",
    "    \"query\", \"gold_answer\", \"golden_context\", \"elapsed_time\"\n",
    "]\n",
    "llm_data = [\n",
    "    \"prediction\", \"prediction_golden\", \"semantic_evaluation\", \"semantic_evaluation_golden\"\n",
    "]\n",
    "llm_metrics = [\n",
    "    \"rougeL\", \"rougeL_golden\", \"bleu\", \"bleu_golden\", \"bleurt\", \"bleurt_golden\",\n",
    "    \"cosine_similarity\", \"cosine_similarity_golden\", \"longest_match\", \"longest_match_golden\"\n",
    "]\n",
    "retrieval_data = [\n",
    "    \"relevant_ids\", \"retrieved_ids\"\n",
    "]\n",
    "retrieval_metrics = [\"precision_dict\", \"recall_dict\", \"map_dict\", \"ndcg_dict\"]\n",
    "\n",
    "# Always include these columns\n",
    "common_cols = [\"index\"]\n",
    "\n",
    "# Save general infos of test run\n",
    "general_infos_df = results_df[common_cols + general_infos]\n",
    "general_infos_json = general_infos_df.to_dict(orient=\"records\")\n",
    "with open(os.path.join(out_path, \"queries.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(general_infos_json, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Save LLM metrics\n",
    "if not only_retrieve:\n",
    "    results_df[llm_metrics].to_csv(os.path.join(out_path, \"generation_metrics.csv\"), index=False)\n",
    "    # Save LLM responses\n",
    "    llm_data_df = results_df[common_cols + llm_data]\n",
    "    general_infos_json = llm_data_df.to_dict(orient=\"records\")\n",
    "    with open(os.path.join(out_path, \"responses.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(general_infos_json, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Save retrieval metrics\n",
    "sums = {m: defaultdict(float) for m in retrieval_metrics}\n",
    "counts = {m: defaultdict(int) for m in retrieval_metrics}\n",
    "\n",
    "for r in results:\n",
    "    for m in retrieval_metrics:\n",
    "        for k, v in r[m].items():\n",
    "            sums[m][k] += float(v)\n",
    "            counts[m][k] += 1\n",
    "\n",
    "means = {\n",
    "    m: {k: (sums[m][k] / counts[m][k]) for k in sums[m]}\n",
    "    for m in retrieval_metrics\n",
    "}\n",
    "\n",
    "means_df = pd.DataFrame()\n",
    "means_df[\"k\"] = config[\"k_values\"]\n",
    "for _, m in means.items():\n",
    "    metric = next(iter(m.keys()))\n",
    "    metric = metric.split(\"@\")[0]\n",
    "    values = m.values()\n",
    "    means_df[metric] = values\n",
    "means_df.to_csv(os.path.join(out_path, \"retrieval_metrics.csv\"), index=False)\n",
    "# Save retrieved docs\n",
    "llm_data_df = results_df[common_cols + retrieval_data]\n",
    "general_infos_json = llm_data_df.to_dict(orient=\"records\")\n",
    "with open(os.path.join(out_path, \"retrieved_docs.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(general_infos_json, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Save config\n",
    "with open(os.path.join(out_path, \"used_config.json\"), \"w\") as f:\n",
    "    json.dump(config, f, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
