{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c8710a6fc0dc5e3",
   "metadata": {},
   "source": [
    "# Data Plots\n",
    "\n",
    "This notebook is designed to visualize the results of various configurations. It reads the results from CSV files, processes them, and generates plots to compare different metrics across configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae946b55f0a4dbb",
   "metadata": {},
   "source": [
    "# Imports and dependencies\n",
    "\n",
    "This section imports the required libraries for data manipulation and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78691453451a4a2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T16:33:53.631735Z",
     "start_time": "2025-07-02T16:33:53.628730Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23905838cec8fae",
   "metadata": {},
   "source": [
    "# Input variables\n",
    "\n",
    "This section defines the base directory where the results are stored and if an llm model has been used during tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4a3914e07ed658",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"output/prova/\"\n",
    "use_llm = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac870ae40fb5228",
   "metadata": {},
   "source": [
    "# Searching for files\n",
    "\n",
    "This section searches for all CSV files that contain results in the specified directory and its subdirectories. The files are expected to be named \"results.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc750982ca5d5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_files = []\n",
    "for root, dirs, files in os.walk(base_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\"results.csv\"):\n",
    "            results_files.append(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1074ec364685cc9",
   "metadata": {},
   "source": [
    "# Reading and processing the data\n",
    "\n",
    "This section reads each results file into a DataFrame, extracts the configuration name from the file path, and appends it to a list. Finally, it concatenates all DataFrames into a single DataFrame for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede6cb71dcb7c1f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T16:33:53.659949Z",
     "start_time": "2025-07-02T16:33:53.643380Z"
    }
   },
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for filepath in results_files:\n",
    "    config_name = os.path.basename(os.path.dirname(filepath))\n",
    "    df = pd.read_csv(filepath)\n",
    "    df[\"config\"] = config_name\n",
    "    df_list.append(df)\n",
    "    \n",
    "combined_df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b143e2ef17dd517",
   "metadata": {},
   "source": [
    "# Metrics to plot\n",
    "\n",
    "This section defines the metrics to be plotted based on whether an LLM model was used during the tests. If `use_llm` is set to `True`, additional metrics related to the LLM are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201bf10d4b9cbf86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T16:33:53.674723Z",
     "start_time": "2025-07-02T16:33:53.660958Z"
    }
   },
   "outputs": [],
   "source": [
    "metrics_to_plot = [\n",
    "    \"rougeL\", \"rougeL_golden\", \n",
    "    \"bleu\", \"bleu_golden\", \n",
    "    \"bleurt\", \"bleurt_golden\", \n",
    "    \"cosine_similarity\", \"cosine_similarity_golden\",\n",
    "    \"longest_match\", \"longest_match_golden\",\n",
    "]\n",
    "\n",
    "mean_metrics = combined_df.groupby(\"config\")[metrics_to_plot].mean().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffa126c00f380e7",
   "metadata": {},
   "source": [
    "# Plotting the metrics\n",
    "\n",
    "This section reshapes the DataFrame for plotting and generates a bar plot to compare the specified metrics across different configurations. The plot is saved as an image file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df = mean_metrics.melt(id_vars=\"config\", var_name=\"Metrics\", value_name=\"value\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=long_df, x=\"config\", y=\"value\", hue=\"Metrics\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.title(\"Metrics Comparison\")\n",
    "plt.tight_layout()\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "file_name = os.path.split(base_dir)[-1]\n",
    "plt.savefig(os.path.join(base_dir, f\"{file_name}_plot.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2bbf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import json\n",
    "\n",
    "responses_path = os.path.join(base_dir, \"responses.json\")\n",
    "\n",
    "with open(responses_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    responses = json.load(f)\n",
    "\n",
    "def show_retrieved_docs(idx):\n",
    "    r = responses[idx]\n",
    "    print(f\"Query [{idx}]: {r['query']}\\n\")\n",
    "    print(f\"Prediction: {r['prediction']}\\n\")\n",
    "    print(\"Retrieved documents:\")\n",
    "    for i, doc in enumerate(r[\"retrieved_docs\"]):\n",
    "        print(f\"  [{i+1}] {doc[:200]}...\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "for idx in range(len(responses)):\n",
    "    show_retrieved_docs(idx)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab59b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "metrics_by_k_path = os.path.join(base_dir, \"metrics_by_k.csv\")\n",
    "if os.path.exists(metrics_by_k_path):\n",
    "    metrics_by_k = pd.read_csv(metrics_by_k_path)\n",
    "    metrics = [\"precision\", \"recall\", \"map\", \"ndcg\"]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for metric in metrics:\n",
    "        plt.plot(metrics_by_k[\"k\"], metrics_by_k[metric], marker=\"o\", label=metric)\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"Retrieval metrics by k\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(base_dir, \"metrics_by_k_plot.png\"), dpi=300)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"File {metrics_by_k_path} non trovato.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
